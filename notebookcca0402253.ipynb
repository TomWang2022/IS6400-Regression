{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12472224,"sourceType":"datasetVersion","datasetId":7868654},{"sourceId":12474860,"sourceType":"datasetVersion","datasetId":7870650}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-17T13:04:11.737237Z","iopub.execute_input":"2025-07-17T13:04:11.737391Z","iopub.status.idle":"2025-07-17T13:04:13.478657Z","shell.execute_reply.started":"2025-07-17T13:04:11.737376Z","shell.execute_reply":"2025-07-17T13:04:13.477905Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/ppm-theory/PPM Theory.csv\n/kaggle/input/intuition-theory-prediction/intuition_and_theory_explanation_prediction.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 0. PPM Thoery-guided 数据集","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/ppm-theory/PPM Theory.csv\")\ndf.head(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T13:04:13.479379Z","iopub.execute_input":"2025-07-17T13:04:13.479783Z","iopub.status.idle":"2025-07-17T13:04:13.533215Z","shell.execute_reply.started":"2025-07-17T13:04:13.479755Z","shell.execute_reply":"2025-07-17T13:04:13.532474Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0.1  Unnamed: 0  city_development_index gender  \\\n0             0           0                   0.920   Male   \n1             1           1                   0.939   Male   \n\n      relevent_experience enrolled_university education_level  \\\n0  No relevent experience    Full time course         Masters   \n1  No relevent experience    Part time course     High School   \n\n  major_discipline experience company_size   company_type last_new_job  \\\n0            Other         17       10000+  Public Sector            1   \n1     not given us          4        10/49        Pvt Ltd            1   \n\n   training_hours  target  Prediction  \\\n0              66     0.0         1.0   \n1              12     0.0         1.0   \n\n                                         Explanation  all_Prediction  \\\n0  Although the candidate has 17 years of working...             0.0   \n1  Although the candidate has 4 years of working ...             1.0   \n\n                                     all_Explanation  \n0  Based on the PushPullMooring theory, I predict...  \n1  Based on the PushPullMooring theory, I predict...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0.1</th>\n      <th>Unnamed: 0</th>\n      <th>city_development_index</th>\n      <th>gender</th>\n      <th>relevent_experience</th>\n      <th>enrolled_university</th>\n      <th>education_level</th>\n      <th>major_discipline</th>\n      <th>experience</th>\n      <th>company_size</th>\n      <th>company_type</th>\n      <th>last_new_job</th>\n      <th>training_hours</th>\n      <th>target</th>\n      <th>Prediction</th>\n      <th>Explanation</th>\n      <th>all_Prediction</th>\n      <th>all_Explanation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.920</td>\n      <td>Male</td>\n      <td>No relevent experience</td>\n      <td>Full time course</td>\n      <td>Masters</td>\n      <td>Other</td>\n      <td>17</td>\n      <td>10000+</td>\n      <td>Public Sector</td>\n      <td>1</td>\n      <td>66</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>Although the candidate has 17 years of working...</td>\n      <td>0.0</td>\n      <td>Based on the PushPullMooring theory, I predict...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0.939</td>\n      <td>Male</td>\n      <td>No relevent experience</td>\n      <td>Part time course</td>\n      <td>High School</td>\n      <td>not given us</td>\n      <td>4</td>\n      <td>10/49</td>\n      <td>Pvt Ltd</td>\n      <td>1</td>\n      <td>12</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>Although the candidate has 4 years of working ...</td>\n      <td>1.0</td>\n      <td>Based on the PushPullMooring theory, I predict...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# ***重新做一遍实验***","metadata":{}},{"cell_type":"markdown","source":"## 0.1 Baseline","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom gensim.models import Word2Vec\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\n# ========== STEP 1: 数据加载与预处理 ==========\n# 用你的实际 DataFrame 替代\ndf = df.copy()  # 假设你的 DataFrame 已经是 df\n\ncategorical_cols = ['gender', 'relevent_experience', 'enrolled_university',\n                    'education_level', 'major_discipline', 'experience',\n                    'company_size', 'company_type','last_new_job']\nnumerical_cols = ['city_development_index','training_hours']\ntarget_col = 'target'\n\n# 填充缺失值\ndf[categorical_cols] = df[categorical_cols].fillna(\"UNKNOWN\")\ndf[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())\n\n# ========== STEP 2: Word2Vec Skip-gram ==========\nw2v_models = {}\nembedding_size = 8  # 小数据建议用小维度\n\nfor col in categorical_cols:\n    sentences = [[str(x)] for x in df[col].values]\n    model = Word2Vec(sentences=sentences, vector_size=embedding_size, window=2, min_count=1, sg=0, epochs=30) # sg 为0时为 CBOW 1时为skip-gram\n    w2v_models[col] = model\n\n# 数值特征标准化\nscaler = StandardScaler()\ndf[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n\n# ========== STEP 3: 构建 BiLSTM 输入序列 ==========\ndef row_to_sequence(row):\n    seq = []\n    for col in categorical_cols:\n        word = str(row[col])\n        vec = w2v_models[col].wv[word]\n        seq.append(vec)\n    for col in numerical_cols:\n        vec = np.full((embedding_size,), row[col])\n        seq.append(vec)\n    return np.stack(seq)  # shape: (len(categorical_cols + numerical_cols), embedding_size)\n\nX_seq = np.stack([row_to_sequence(row) for _, row in df.iterrows()])\ny = df[target_col].values\n\n# ========== STEP 4: 数据划分 ==========\nX_train, X_test, y_train, y_test = train_test_split(X_seq, y, test_size=0.3, random_state=42\n                                        #            ,stratify=y\n                                                   )\n\n# ========== STEP 5: PyTorch Dataset 和 Dataloader ==========\nclass SequenceDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self): return len(self.X)\n    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n\ntrain_loader = DataLoader(SequenceDataset(X_train, y_train), batch_size=16, shuffle=True)\ntest_loader = DataLoader(SequenceDataset(X_test, y_test), batch_size=16)\n\n# ========== STEP 6: Bi-LSTM 模型 ==========\nclass BiLSTMClassifier(nn.Module):\n    def __init__(self, input_dim=8, hidden_dim=64, output_dim=2, num_layers=1, dropout=0.1):\n        super().__init__()\n        self.bilstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=True,\n            dropout=dropout if num_layers > 1 else 0.0\n        )\n        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # 因为是双向，所以乘2\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        # 输入 x 形状: (batch, seq_len, input_dim)\n        lstm_out, _ = self.bilstm(x)        # (batch, seq_len, hidden_dim*2)\n        x = lstm_out[:, -1, :]              # 取最后时间步的输出\n        x = self.relu(x)\n        return self.fc(x)                   # (batch, output_dim)\n\n\nmodel = BiLSTMClassifier(input_dim=8, output_dim=2)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n\n\n\n\n# ========== STEP 7: 训练 ==========\nfor epoch in range(30):\n    model.train()\n    total_loss = 0\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        logits = model(X_batch)\n        loss = criterion(logits, y_batch)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\") # 此处打印的为Training loss \n\n# ========== STEP 8: 评估 ==========\nimport torch.nn.functional as F\n\nmodel.eval()\ny_true, y_pred, y_score = [], [], []\n\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        logits = model(X_batch)\n\n        # 获取预测标签（0或1）\n        preds = torch.argmax(logits, dim=1)\n        y_pred.extend(preds.cpu().numpy())\n\n        # 获取正类的概率值，假设是二分类，正类索引为 1\n        probs = F.softmax(logits, dim=1)[:, 1]\n        y_score.extend(probs.cpu().numpy())\n\n        y_true.extend(y_batch.cpu().numpy())\n\n\nfrom sklearn.metrics import (\n    precision_score, recall_score, f1_score, accuracy_score,\n    confusion_matrix, roc_auc_score\n)\n\ndef evaluate(y_true, y_pred, y_score=None):\n    precision = precision_score(y_true, y_pred, zero_division=0)\n    recall = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    acc = accuracy_score(y_true, y_pred)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    fpr = fp / (fp + tn + 1e-8)\n    pnr = fn / (fn + tp + 1e-8)\n    auc = roc_auc_score(y_true, y_score) if y_score is not None else None\n    return precision, recall, f1, acc, fpr, pnr, auc\n\n# 示例调用\nmetrics = evaluate(y_true, y_pred, y_score)\nnames = ['Precision', 'Recall', 'F1', 'Accuracy', 'FPR', 'PNR', 'AUC']\nfor n, v in zip(names, metrics):\n    print(f\"{n}: {v:.4f}\" if v is not None else f\"{n}: N/A\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T13:45:05.762777Z","iopub.execute_input":"2025-07-17T13:45:05.763342Z","iopub.status.idle":"2025-07-17T13:45:07.195242Z","shell.execute_reply.started":"2025-07-17T13:45:05.763317Z","shell.execute_reply":"2025-07-17T13:45:07.194528Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 6.2428\nEpoch 2, Loss: 6.1981\nEpoch 3, Loss: 6.1547\nEpoch 4, Loss: 6.1151\nEpoch 5, Loss: 6.0669\nEpoch 6, Loss: 5.9957\nEpoch 7, Loss: 5.9373\nEpoch 8, Loss: 5.8517\nEpoch 9, Loss: 5.7407\nEpoch 10, Loss: 5.6344\nEpoch 11, Loss: 5.5497\nEpoch 12, Loss: 5.4330\nEpoch 13, Loss: 5.3167\nEpoch 14, Loss: 5.2863\nEpoch 15, Loss: 5.2299\nEpoch 16, Loss: 5.2176\nEpoch 17, Loss: 5.1878\nEpoch 18, Loss: 5.1758\nEpoch 19, Loss: 5.1104\nEpoch 20, Loss: 5.1101\nEpoch 21, Loss: 5.1237\nEpoch 22, Loss: 5.1438\nEpoch 23, Loss: 5.1587\nEpoch 24, Loss: 5.0581\nEpoch 25, Loss: 5.0963\nEpoch 26, Loss: 5.0454\nEpoch 27, Loss: 5.0149\nEpoch 28, Loss: 5.0535\nEpoch 29, Loss: 5.0863\nEpoch 30, Loss: 5.0525\nPrecision: 0.7333\nRecall: 0.3793\nF1: 0.5000\nAccuracy: 0.6333\nFPR: 0.1290\nPNR: 0.6207\nAUC: 0.6385\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## 0.2 所有设计在一起","metadata":{}},{"cell_type":"code","source":"# STEP 0: 引入 BERT 模块和 PCA\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.decomposition import PCA\nimport torch\nimport numpy as np\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\nfrom tqdm import tqdm\n\n# STEP 1: 加载 BERT 模型（一次性加载）\nbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nbert_model = BertModel.from_pretrained(\"bert-base-uncased\")\nbert_model.eval()  # 不训练\n\n# STEP 2: 获取 Explanation 的 BERT 向量\n@torch.no_grad()\ndef get_bert_embedding(text):\n    inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    outputs = bert_model(**inputs)\n    # 使用 [CLS] 向量作为整句表示\n    cls_embedding = outputs.last_hidden_state[:, 0, :]  # shape: [1, 768]\n    return cls_embedding.squeeze().numpy()  # shape: [768,]\n\n# 生成所有 Explanation 的 BERT 向量\nexplanation_embeddings = np.stack([get_bert_embedding(str(x)) for x in tqdm(df['all_Explanation'])])\n\n# STEP 3: 使用 PCA 降维将 BERT 向量从 768 维降到 8 维\npca = PCA(n_components=8)\nexplanation_embeddings_8d = pca.fit_transform(explanation_embeddings)  # shape: (200, 8)\n\n# STEP 4: Word2Vec 和数值特征的序列化\ndef row_to_sequence(row):\n    seq = []\n    for col in categorical_cols:\n        word = str(row[col])\n        vec = w2v_models[col].wv[word]\n        seq.append(vec)\n    for col in numerical_cols:\n        vec = np.full((embedding_size,), row[col])\n        seq.append(vec)\n    return np.stack(seq)  # shape: (len(categorical_cols + numerical_cols), embedding_size)\n\nX_seq_with_exp = []\n\nfor i in range(len(df)):\n    original_seq = row_to_sequence(df.iloc[i])  # shape: (seq_len, 8)\n    exp_vec_8d = explanation_embeddings_8d[i].reshape(1, -1)  # shape: (1, 8)\n    full_seq = np.concatenate([original_seq, exp_vec_8d], axis=0)  # shape: (seq_len + 1, 8) # 注意此处是直接进行拼接\n    X_seq_with_exp.append(full_seq)\n\nX_seq_with_exp = np.stack(X_seq_with_exp)\ny = df[target_col].values\n\n# STEP 5: 划分训练集\nX_train, X_test, y_train, y_test = train_test_split(X_seq_with_exp, y, test_size=0.3, random_state=42\n                                            #       , stratify=y\n                                                   )\n\n# STEP 6: 新的 Dataset 和 Dataloader\nclass SequenceDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self): return len(self.X)\n    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n\ntrain_loader = DataLoader(SequenceDataset(X_train, y_train), batch_size=16, shuffle=True)\ntest_loader = DataLoader(SequenceDataset(X_test, y_test), batch_size=16)\n\n# # ========== STEP 7: BiLSTM ==========\n\nimport torch.nn as nn\n\nclass BiLSTMWithBert(nn.Module):\n    def __init__(self, input_dim=8, exp_dim=8, hidden_dim=32, output_dim=2, num_layers=1):\n        super().__init__()\n        self.bilstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=True\n        )\n        self.linear_exp = nn.Linear(exp_dim, hidden_dim * 2)\n        self.fc = nn.Linear(hidden_dim * 4, output_dim)\n\n    def forward(self, x):\n        x_seq = x[:, :-1, :]   # (batch, seq_len, input_dim)\n        x_exp = x[:, -1, :]    # (batch, exp_dim)\n\n        # BiLSTM outputs\n        lstm_out, _ = self.bilstm(x_seq)      # (batch, seq_len, hidden_dim*2)\n        lstm_feat = lstm_out.mean(dim=1)      # Mean pooling over time\n\n        exp_feat = self.linear_exp(x_exp)     # (batch, hidden_dim*2)\n\n        combined = torch.cat([lstm_feat, exp_feat], dim=1)  # (batch, hidden_dim*4)\n        return self.fc(combined)\n\nmodel = BiLSTMWithBert(input_dim=8, exp_dim=8, hidden_dim=32, output_dim=2)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n\n# 定义对比损失regularizer \ndef info_nce_loss(features, labels, temperature=0.5):\n    \"\"\"\n    features: tensor of shape (batch_size, embed_dim)\n    labels: tensor of shape (batch_size,) with class labels\n    \"\"\"\n    # Normalize features\n    features = F.normalize(features, dim=1)  # cosine sim\n    similarity_matrix = torch.matmul(features, features.T)  # (batch, batch)\n\n    # Build labels mask (positive pairs: same label)\n    labels = labels.contiguous().view(-1, 1)\n    mask = torch.eq(labels, labels.T).float().to(features.device)  # (batch, batch)\n\n    # Avoid self-similarity\n    self_mask = torch.eye(mask.shape[0], device=features.device)\n    mask = mask - self_mask  # 1 for positive pairs only, 0 elsewhere\n\n    # Scale similarity\n    logits = similarity_matrix / temperature\n\n    # For numerical stability\n    logits_max, _ = torch.max(logits, dim=1, keepdim=True)\n    logits = logits - logits_max.detach()\n\n    exp_logits = torch.exp(logits) * (1 - self_mask)\n    log_prob = logits - torch.log(exp_logits.sum(dim=1, keepdim=True) + 1e-8)\n\n    # Compute mean of log-likelihood over positive\n    mean_log_prob_pos = (mask * log_prob).sum(dim=1) / (mask.sum(dim=1) + 1e-8)\n\n    loss = -mean_log_prob_pos.mean()\n    return loss\n\nimport torch.nn.functional as F\n\ndef cosine_similarity_matrix(features):\n    \"\"\"计算归一化后的余弦相似度矩阵\"\"\"\n    features = F.normalize(features, dim=1)\n    return torch.matmul(features, features.T)\n\nfor epoch in range(40):\n    model.train()\n    total_loss = 0\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        logits = model(X_batch)\n\n        # 原始分类损失\n        loss_cls = criterion(logits, y_batch)\n\n        # 中间表示提取\n        x_seq = X_batch[:, :-1, :]\n        x_exp = X_batch[:, -1, :]\n\n        # 原始序列平均池化作为 baseline 特征\n        original_feat = x_seq.mean(dim=1)  # (batch, input_dim)\n\n        # 模型的中间输出\n        lstm_out, _ = model.bilstm(x_seq)\n        lstm_feat = lstm_out.mean(dim=1)  # (batch, hidden_dim*2)\n        exp_feat = model.linear_exp(x_exp)  # (batch, hidden_dim*2)\n        fused_feat = torch.cat([lstm_feat, exp_feat], dim=1)  # (batch, hidden_dim*4)\n\n        # 对比损失 (InfoNCE)\n        loss_contrast = info_nce_loss(fused_feat, y_batch)\n\n        # 一致性惩罚项\n        sim_orig = cosine_similarity_matrix(original_feat)\n        sim_fused = cosine_similarity_matrix(fused_feat)\n        # 只保留非对角线项（mask掉自己和自己）\n        batch_size = sim_orig.shape[0]\n        mask = ~torch.eye(batch_size, dtype=bool, device=sim_orig.device)\n        loss_consistency = torch.abs(sim_orig - sim_fused)[mask].mean()\n\n        # 总损失\n        alpha = 0.05  # 对比损失系数 0.05\n        beta = 0.05    # 一致性损失系数，可调节 0.1\n        loss = loss_cls + alpha * loss_contrast + beta * loss_consistency\n        # alpha=0.01, beta=0.0 => Avg F1: 0.6486 交叉验证寻求最有组合\n\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n\n\n# STEP 9: 评估\nimport torch.nn.functional as F\n\nmodel.eval()\ny_true, y_pred, y_score = [], [], []\n\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        logits = model(X_batch)\n\n        # 获取预测标签（0或1）\n        preds = torch.argmax(logits, dim=1)\n        y_pred.extend(preds.cpu().numpy())\n\n        # 获取正类的概率值，假设是二分类，正类索引为 1\n        probs = F.softmax(logits, dim=1)[:, 1]\n        y_score.extend(probs.cpu().numpy())\n\n        y_true.extend(y_batch.cpu().numpy())\n\n\nfrom sklearn.metrics import (\n    precision_score, recall_score, f1_score, accuracy_score,\n    confusion_matrix, roc_auc_score\n)\n\ndef evaluate(y_true, y_pred, y_score=None):\n    precision = precision_score(y_true, y_pred, zero_division=0)\n    recall = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    acc = accuracy_score(y_true, y_pred)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    fpr = fp / (fp + tn + 1e-8)\n    pnr = fn / (fn + tp + 1e-8)\n    auc = roc_auc_score(y_true, y_score) if y_score is not None else None\n    return precision, recall, f1, acc, fpr, pnr, auc\n\n# 示例调用\nmetrics = evaluate(y_true, y_pred, y_score)\nnames = ['Precision', 'Recall', 'F1', 'Accuracy', 'FPR', 'PNR', 'AUC']\nfor n, v in zip(names, metrics):\n    print(f\"{n}: {v:.4f}\" if v is not None else f\"{n}: N/A\")\n\n# 训练直至收敛","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T13:54:43.354810Z","iopub.execute_input":"2025-07-17T13:54:43.355325Z","iopub.status.idle":"2025-07-17T13:55:17.035956Z","shell.execute_reply.started":"2025-07-17T13:54:43.355299Z","shell.execute_reply":"2025-07-17T13:55:17.035350Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 200/200 [00:29<00:00,  6.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 7.7421\nEpoch 2, Loss: 7.5244\nEpoch 3, Loss: 7.3765\nEpoch 4, Loss: 7.2548\nEpoch 5, Loss: 7.2118\nEpoch 6, Loss: 7.1145\nEpoch 7, Loss: 7.0518\nEpoch 8, Loss: 7.0464\nEpoch 9, Loss: 6.9695\nEpoch 10, Loss: 6.9279\nEpoch 11, Loss: 6.8296\nEpoch 12, Loss: 6.7721\nEpoch 13, Loss: 6.7114\nEpoch 14, Loss: 6.6480\nEpoch 15, Loss: 6.6091\nEpoch 16, Loss: 6.5524\nEpoch 17, Loss: 6.5497\nEpoch 18, Loss: 6.5149\nEpoch 19, Loss: 6.5123\nEpoch 20, Loss: 6.5024\nEpoch 21, Loss: 6.4837\nEpoch 22, Loss: 6.4190\nEpoch 23, Loss: 6.4359\nEpoch 24, Loss: 6.4170\nEpoch 25, Loss: 6.4175\nEpoch 26, Loss: 6.3762\nEpoch 27, Loss: 6.3263\nEpoch 28, Loss: 6.4221\nEpoch 29, Loss: 6.3211\nEpoch 30, Loss: 6.3387\nEpoch 31, Loss: 6.3133\nEpoch 32, Loss: 6.4599\nEpoch 33, Loss: 6.3118\nEpoch 34, Loss: 6.3600\nEpoch 35, Loss: 6.3852\nEpoch 36, Loss: 6.3185\nEpoch 37, Loss: 6.3311\nEpoch 38, Loss: 6.3373\nEpoch 39, Loss: 6.3644\nEpoch 40, Loss: 6.3066\nPrecision: 0.7407\nRecall: 0.6897\nF1: 0.7143\nAccuracy: 0.7333\nFPR: 0.2258\nPNR: 0.3103\nAUC: 0.7519\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# 每次训练，测试数据不同都需要不同的调整参数（炼丹）","metadata":{}},{"cell_type":"markdown","source":"## Candidate-Enhancer w/o cross-attention","metadata":{},"attachments":{"ccc84a36-a508-42d7-859d-faf103ad60ba.png":{"image/png":"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"},"5823a80b-0054-4b7a-825b-eaeaec206cff.png":{"image/png":"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"},"75f4073e-6d88-40ff-bf5b-b255a01babff.png":{"image/png":"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"},"0b003f2f-7f77-4ab4-a73b-e772fe41588f.png":{"image/png":"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"}}},{"cell_type":"code","source":"# STEP 0: 引入 BERT 模块和 PCA\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.decomposition import PCA\nimport torch\nimport numpy as np\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\nfrom tqdm import tqdm\n\n# STEP 1: 加载 BERT 模型（一次性加载）\nbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nbert_model = BertModel.from_pretrained(\"bert-base-uncased\")\nbert_model.eval()  # 不训练\n\n# STEP 2: 获取 Explanation 的 BERT 向量\n@torch.no_grad()\ndef get_bert_embedding(text):\n    inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    outputs = bert_model(**inputs)\n    # 使用 [CLS] 向量作为整句表示\n    cls_embedding = outputs.last_hidden_state[:, 0, :]  # shape: [1, 768]\n    return cls_embedding.squeeze().numpy()  # shape: [768,]\n\n# 生成所有 Explanation 的 BERT 向量\nexplanation_embeddings = np.stack([get_bert_embedding(str(x)) for x in tqdm(df['all_Explanation'])])\n\n# STEP 3: 使用 PCA 降维将 BERT 向量从 768 维降到 8 维\npca = PCA(n_components=8)\nexplanation_embeddings_8d = pca.fit_transform(explanation_embeddings)  # shape: (200, 8)\n\n# STEP 4: Word2Vec 和数值特征的序列化\ndef row_to_sequence(row):\n    seq = []\n    for col in categorical_cols:\n        word = str(row[col])\n        vec = w2v_models[col].wv[word]\n        seq.append(vec)\n    for col in numerical_cols:\n        vec = np.full((embedding_size,), row[col])\n        seq.append(vec)\n    return np.stack(seq)  # shape: (len(categorical_cols + numerical_cols), embedding_size)\n\n# X_seq_with_exp = []\n\n# for i in range(len(df)):\n#     original_seq = row_to_sequence(df.iloc[i])  # shape: (seq_len, 8)\n#     exp_vec_8d = explanation_embeddings_8d[i].reshape(1, -1)  # shape: (1, 8)\n#     full_seq = np.concatenate([original_seq, exp_vec_8d], axis=0)  # shape: (seq_len + 1, 8) # 注意此处是直接进行拼接\n#     X_seq_with_exp.append(full_seq)\n\n# X_seq_with_exp = np.stack(X_seq_with_exp)\n# y = df[target_col].values\n\n# 选项2 使用 Cross-attention 融合\nimport torch\nimport torch.nn as nn\n\n# 定义 Cross-Attention 模块\nclass CrossAttentionFusion(nn.Module):\n    def __init__(self, embed_dim=8, num_heads=1):\n        super(CrossAttentionFusion, self).__init__()\n        self.cross_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n        self.norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, query_vec, context_seq):\n        # query_vec: (B, 1, E)\n        # context_seq: (B, L, E)\n        attn_output, _ = self.cross_attn(query_vec, context_seq, context_seq)\n#        attn_output, _ = self.cross_attn(context_seq, query_vec, query_vec) 效果不好\n        fused = self.norm(query_vec + attn_output)\n        return fused  # shape: (B, 1, E)\n\n# 初始化模块（可以放在外部）\ncross_attention = CrossAttentionFusion(embed_dim=8, num_heads=1)\n\n# 融合序列和解释向量\nX_seq_with_exp = []\n\nfor i in range(len(df)):\n    row = df.iloc[i]\n    \n    original_seq = row_to_sequence(row)  # shape: (seq_len, 8)\n    exp_vec_8d = explanation_embeddings_8d[i].reshape(1, 8)  # shape: (1, 8)\n\n    # 转换为 tensor\n    context_seq = torch.tensor(original_seq, dtype=torch.float32).unsqueeze(0)  # shape: (1, seq_len, 8)\n    query_vec = torch.tensor(exp_vec_8d, dtype=torch.float32).unsqueeze(0)      # shape: (1, 1, 8)\n\n    # 运行 cross-attention\n    fused_vec = cross_attention(query_vec, context_seq)  # shape: (1, 1, 8)\n    fused_vec_np = fused_vec.squeeze(0).detach().numpy()  # shape: (1, 8)\n\n    # 选择如何使用融合后的向量：拼接 or 替换\n    full_seq = np.concatenate([original_seq, fused_vec_np], axis=0)  # shape: (seq_len + 1, 8)\n\n    X_seq_with_exp.append(full_seq)\n\nX_seq_with_exp = np.stack(X_seq_with_exp)  # shape: (num_samples, seq_len + 1, 8)\ny = df[target_col].values\n\n# STEP 5: 划分训练集\nX_train, X_test, y_train, y_test = train_test_split(X_seq_with_exp, y, test_size=0.3, random_state=42\n                                            #       , stratify=y\n                                                   )\n\n# STEP 6: 新的 Dataset 和 Dataloader\nclass SequenceDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self): return len(self.X)\n    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n\ntrain_loader = DataLoader(SequenceDataset(X_train, y_train), batch_size=16, shuffle=True)\ntest_loader = DataLoader(SequenceDataset(X_test, y_test), batch_size=16)\n\n# # ========== STEP 7: BiLSTM ==========\n\nimport torch.nn as nn\n\nclass BiLSTMWithBert(nn.Module):\n    def __init__(self, input_dim=8, exp_dim=8, hidden_dim=32, output_dim=2, num_layers=1):\n        super().__init__()\n        self.bilstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=True\n        )\n        self.linear_exp = nn.Linear(exp_dim, hidden_dim * 2)\n        self.fc = nn.Linear(hidden_dim * 4, output_dim)\n\n    def forward(self, x):\n        x_seq = x[:, :-1, :]   # (batch, seq_len, input_dim)\n        x_exp = x[:, -1, :]    # (batch, exp_dim)\n\n        # BiLSTM outputs\n        lstm_out, _ = self.bilstm(x_seq)      # (batch, seq_len, hidden_dim*2)\n        lstm_feat = lstm_out.mean(dim=1)      # Mean pooling over time\n\n        exp_feat = self.linear_exp(x_exp)     # (batch, hidden_dim*2)\n\n        combined = torch.cat([lstm_feat, exp_feat], dim=1)  # (batch, hidden_dim*4)\n        return self.fc(combined)\n\nmodel = BiLSTMWithBert(input_dim=8, exp_dim=8, hidden_dim=32, output_dim=2)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n\n# 定义对比损失regularizer \ndef info_nce_loss(features, labels, temperature=0.5):\n    \"\"\"\n    features: tensor of shape (batch_size, embed_dim)\n    labels: tensor of shape (batch_size,) with class labels\n    \"\"\"\n    # Normalize features\n    features = F.normalize(features, dim=1)  # cosine sim\n    similarity_matrix = torch.matmul(features, features.T)  # (batch, batch)\n\n    # Build labels mask (positive pairs: same label)\n    labels = labels.contiguous().view(-1, 1)\n    mask = torch.eq(labels, labels.T).float().to(features.device)  # (batch, batch)\n\n    # Avoid self-similarity\n    self_mask = torch.eye(mask.shape[0], device=features.device)\n    mask = mask - self_mask  # 1 for positive pairs only, 0 elsewhere\n\n    # Scale similarity\n    logits = similarity_matrix / temperature\n\n    # For numerical stability\n    logits_max, _ = torch.max(logits, dim=1, keepdim=True)\n    logits = logits - logits_max.detach()\n\n    exp_logits = torch.exp(logits) * (1 - self_mask)\n    log_prob = logits - torch.log(exp_logits.sum(dim=1, keepdim=True) + 1e-8)\n\n    # Compute mean of log-likelihood over positive\n    mean_log_prob_pos = (mask * log_prob).sum(dim=1) / (mask.sum(dim=1) + 1e-8)\n\n    loss = -mean_log_prob_pos.mean()\n    return loss\n\nimport torch.nn.functional as F\n\ndef cosine_similarity_matrix(features):\n    \"\"\"计算归一化后的余弦相似度矩阵\"\"\"\n    features = F.normalize(features, dim=1)\n    return torch.matmul(features, features.T)\n\nfor epoch in range(30):\n    model.train()\n    total_loss = 0\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        logits = model(X_batch)\n\n        # 原始分类损失\n        loss_cls = criterion(logits, y_batch)\n\n        # 中间表示提取\n        x_seq = X_batch[:, :-1, :]\n        x_exp = X_batch[:, -1, :]\n\n        # 原始序列平均池化作为 baseline 特征\n        original_feat = x_seq.mean(dim=1)  # (batch, input_dim)\n\n        # 模型的中间输出\n        lstm_out, _ = model.bilstm(x_seq)\n        lstm_feat = lstm_out.mean(dim=1)  # (batch, hidden_dim*2)\n        exp_feat = model.linear_exp(x_exp)  # (batch, hidden_dim*2)\n        fused_feat = torch.cat([lstm_feat, exp_feat], dim=1)  # (batch, hidden_dim*4)\n\n        # 对比损失 (InfoNCE)\n        loss_contrast = info_nce_loss(fused_feat, y_batch)\n\n        # 一致性惩罚项\n        sim_orig = cosine_similarity_matrix(original_feat)\n        sim_fused = cosine_similarity_matrix(fused_feat)\n        # 只保留非对角线项（mask掉自己和自己）\n        batch_size = sim_orig.shape[0]\n        mask = ~torch.eye(batch_size, dtype=bool, device=sim_orig.device)\n        loss_consistency = torch.abs(sim_orig - sim_fused)[mask].mean()\n\n        # 总损失\n        alpha = 0.05  # 对比损失系数 0.05\n        beta = 0.05    # 一致性损失系数，可调节 0.1\n        loss = loss_cls + alpha * loss_contrast + beta * loss_consistency\n        # alpha=0.01, beta=0.0 => Avg F1: 0.6486 交叉验证寻求最有组合\n\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n\n\n# STEP 9: 评估\nimport torch.nn.functional as F\n\nmodel.eval()\ny_true, y_pred, y_score = [], [], []\n\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        logits = model(X_batch)\n\n        # 获取预测标签（0或1）\n        preds = torch.argmax(logits, dim=1)\n        y_pred.extend(preds.cpu().numpy())\n\n        # 获取正类的概率值，假设是二分类，正类索引为 1\n        probs = F.softmax(logits, dim=1)[:, 1]\n        y_score.extend(probs.cpu().numpy())\n\n        y_true.extend(y_batch.cpu().numpy())\n\n\nfrom sklearn.metrics import (\n    precision_score, recall_score, f1_score, accuracy_score,\n    confusion_matrix, roc_auc_score\n)\n\ndef evaluate(y_true, y_pred, y_score=None):\n    precision = precision_score(y_true, y_pred, zero_division=0)\n    recall = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    acc = accuracy_score(y_true, y_pred)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    fpr = fp / (fp + tn + 1e-8)\n    pnr = fn / (fn + tp + 1e-8)\n    auc = roc_auc_score(y_true, y_score) if y_score is not None else None\n    return precision, recall, f1, acc, fpr, pnr, auc\n\n# 示例调用\nmetrics = evaluate(y_true, y_pred, y_score)\nnames = ['Precision', 'Recall', 'F1', 'Accuracy', 'FPR', 'PNR', 'AUC']\nfor n, v in zip(names, metrics):\n    print(f\"{n}: {v:.4f}\" if v is not None else f\"{n}: N/A\")\n\n# 训练直至收敛","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T13:12:39.452785Z","iopub.execute_input":"2025-07-17T13:12:39.453744Z","iopub.status.idle":"2025-07-17T13:13:12.891260Z","shell.execute_reply.started":"2025-07-17T13:12:39.453717Z","shell.execute_reply":"2025-07-17T13:13:12.890461Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 200/200 [00:29<00:00,  6.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 8.5799\nEpoch 2, Loss: 8.2054\nEpoch 3, Loss: 7.9707\nEpoch 4, Loss: 7.7622\nEpoch 5, Loss: 7.6103\nEpoch 6, Loss: 7.4538\nEpoch 7, Loss: 7.3793\nEpoch 8, Loss: 7.2558\nEpoch 9, Loss: 7.2011\nEpoch 10, Loss: 7.1640\nEpoch 11, Loss: 7.0095\nEpoch 12, Loss: 7.0241\nEpoch 13, Loss: 6.9014\nEpoch 14, Loss: 6.7307\nEpoch 15, Loss: 6.7287\nEpoch 16, Loss: 6.5954\nEpoch 17, Loss: 6.5172\nEpoch 18, Loss: 6.6050\nEpoch 19, Loss: 6.5041\nEpoch 20, Loss: 6.4779\nEpoch 21, Loss: 6.5760\nEpoch 22, Loss: 6.4663\nEpoch 23, Loss: 6.4324\nEpoch 24, Loss: 6.4891\nEpoch 25, Loss: 6.4668\nEpoch 26, Loss: 6.4576\nEpoch 27, Loss: 6.4440\nEpoch 28, Loss: 6.4087\nEpoch 29, Loss: 6.4341\nEpoch 30, Loss: 6.3993\nPrecision: 0.6923\nRecall: 0.6207\nF1: 0.6545\nAccuracy: 0.6833\nFPR: 0.2581\nPNR: 0.3793\nAUC: 0.7419\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# 1. Baseline ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom gensim.models import Word2Vec\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\n# ========== STEP 1: 数据加载与预处理 ==========\n# 用你的实际 DataFrame 替代\ndf = df.copy()  # 假设你的 DataFrame 已经是 df\n\ncategorical_cols = ['gender', 'relevent_experience', 'enrolled_university',\n                    'education_level', 'major_discipline', 'experience',\n                    'company_size', 'company_type','last_new_job']\nnumerical_cols = ['city_development_index','training_hours']\ntarget_col = 'target'\n\n# 填充缺失值\ndf[categorical_cols] = df[categorical_cols].fillna(\"UNKNOWN\")\ndf[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())\n\n# ========== STEP 2: Word2Vec Skip-gram ==========\nw2v_models = {}\nembedding_size = 8  # 小数据建议用小维度\n\nfor col in categorical_cols:\n    sentences = [[str(x)] for x in df[col].values]\n    model = Word2Vec(sentences=sentences, vector_size=embedding_size, window=2, min_count=1, sg=0, epochs=30) # sg 为0时为 CBOW 1时为skip-gram\n    w2v_models[col] = model\n\n# 数值特征标准化\nscaler = StandardScaler()\ndf[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n\n# ========== STEP 3: 构建 BiLSTM 输入序列 ==========\ndef row_to_sequence(row):\n    seq = []\n    for col in categorical_cols:\n        word = str(row[col])\n        vec = w2v_models[col].wv[word]\n        seq.append(vec)\n    for col in numerical_cols:\n        vec = np.full((embedding_size,), row[col])\n        seq.append(vec)\n    return np.stack(seq)  # shape: (len(categorical_cols + numerical_cols), embedding_size)\n\nX_seq = np.stack([row_to_sequence(row) for _, row in df.iterrows()])\ny = df[target_col].values\n\n# ========== STEP 4: 数据划分 ==========\nX_train, X_test, y_train, y_test = train_test_split(X_seq, y, test_size=0.3, random_state=42\n                                        #            ,stratify=y\n                                                   )\n\n# ========== STEP 5: PyTorch Dataset 和 Dataloader ==========\nclass SequenceDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self): return len(self.X)\n    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n\ntrain_loader = DataLoader(SequenceDataset(X_train, y_train), batch_size=16, shuffle=True)\ntest_loader = DataLoader(SequenceDataset(X_test, y_test), batch_size=16)\n\n# ========== STEP 6: Bi-LSTM 模型 ==========\nclass BiLSTMClassifier(nn.Module):\n    def __init__(self, input_dim=8, hidden_dim=64, output_dim=2, num_layers=1, dropout=0.1):\n        super().__init__()\n        self.bilstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=True,\n            dropout=dropout if num_layers > 1 else 0.0\n        )\n        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # 因为是双向，所以乘2\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        # 输入 x 形状: (batch, seq_len, input_dim)\n        lstm_out, _ = self.bilstm(x)        # (batch, seq_len, hidden_dim*2)\n        x = lstm_out[:, -1, :]              # 取最后时间步的输出\n        x = self.relu(x)\n        return self.fc(x)                   # (batch, output_dim)\n\n\nmodel = BiLSTMClassifier(input_dim=8, output_dim=2)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n\n\n\n\n# ========== STEP 7: 训练 ==========\nfor epoch in range(30):\n    model.train()\n    total_loss = 0\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        logits = model(X_batch)\n        loss = criterion(logits, y_batch)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\") # 此处打印的为Training loss \n\n# ========== STEP 8: 评估 ==========\nimport torch.nn.functional as F\n\nmodel.eval()\ny_true, y_pred, y_score = [], [], []\n\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        logits = model(X_batch)\n\n        # 获取预测标签（0或1）\n        preds = torch.argmax(logits, dim=1)\n        y_pred.extend(preds.cpu().numpy())\n\n        # 获取正类的概率值，假设是二分类，正类索引为 1\n        probs = F.softmax(logits, dim=1)[:, 1]\n        y_score.extend(probs.cpu().numpy())\n\n        y_true.extend(y_batch.cpu().numpy())\n\n\nfrom sklearn.metrics import (\n    precision_score, recall_score, f1_score, accuracy_score,\n    confusion_matrix, roc_auc_score\n)\n\ndef evaluate(y_true, y_pred, y_score=None):\n    precision = precision_score(y_true, y_pred, zero_division=0)\n    recall = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    acc = accuracy_score(y_true, y_pred)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    fpr = fp / (fp + tn + 1e-8)\n    pnr = fn / (fn + tp + 1e-8)\n    auc = roc_auc_score(y_true, y_score) if y_score is not None else None\n    return precision, recall, f1, acc, fpr, pnr, auc\n\n# 示例调用\nmetrics = evaluate(y_true, y_pred, y_score)\nnames = ['Precision', 'Recall', 'F1', 'Accuracy', 'FPR', 'PNR', 'AUC']\nfor n, v in zip(names, metrics):\n    print(f\"{n}: {v:.4f}\" if v is not None else f\"{n}: N/A\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T04:47:05.693193Z","iopub.execute_input":"2025-07-17T04:47:05.693506Z","iopub.status.idle":"2025-07-17T04:47:07.178127Z","shell.execute_reply.started":"2025-07-17T04:47:05.693485Z","shell.execute_reply":"2025-07-17T04:47:07.177399Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 6.2368\nEpoch 2, Loss: 6.1730\nEpoch 3, Loss: 6.1131\nEpoch 4, Loss: 6.0579\nEpoch 5, Loss: 5.9595\nEpoch 6, Loss: 5.8975\nEpoch 7, Loss: 5.7862\nEpoch 8, Loss: 5.6546\nEpoch 9, Loss: 5.5637\nEpoch 10, Loss: 5.4869\nEpoch 11, Loss: 5.3601\nEpoch 12, Loss: 5.2809\nEpoch 13, Loss: 5.1856\nEpoch 14, Loss: 5.1889\nEpoch 15, Loss: 5.1639\nEpoch 16, Loss: 5.1060\nEpoch 17, Loss: 5.1419\nEpoch 18, Loss: 5.1356\nEpoch 19, Loss: 5.1116\nEpoch 20, Loss: 5.0973\nEpoch 21, Loss: 5.0917\nEpoch 22, Loss: 5.0466\nEpoch 23, Loss: 5.0821\nEpoch 24, Loss: 5.0554\nEpoch 25, Loss: 5.1115\nEpoch 26, Loss: 5.1605\nEpoch 27, Loss: 5.0819\nEpoch 28, Loss: 5.1336\nEpoch 29, Loss: 5.0309\nEpoch 30, Loss: 5.0089\nPrecision: 0.7500\nRecall: 0.4138\nF1: 0.5333\nAccuracy: 0.6500\nFPR: 0.1290\nPNR: 0.5862\nAUC: 0.6429\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# 2. LLM-enhanced","metadata":{}},{"cell_type":"code","source":"# STEP 0: 引入 BERT 模块和 PCA\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.decomposition import PCA\nimport torch\nimport numpy as np\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\nfrom tqdm import tqdm\n\n# STEP 1: 加载 BERT 模型（一次性加载）\nbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nbert_model = BertModel.from_pretrained(\"bert-base-uncased\")\nbert_model.eval()  # 不训练\n\n# STEP 2: 获取 Explanation 的 BERT 向量\n@torch.no_grad()\ndef get_bert_embedding(text):\n    inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    outputs = bert_model(**inputs)\n    # 使用 [CLS] 向量作为整句表示\n    cls_embedding = outputs.last_hidden_state[:, 0, :]  # shape: [1, 768]\n    return cls_embedding.squeeze().numpy()  # shape: [768,]\n\n# 生成所有 Explanation 的 BERT 向量\nexplanation_embeddings = np.stack([get_bert_embedding(str(x)) for x in tqdm(df['Explanation'])])\n\n# STEP 3: 使用 PCA 降维将 BERT 向量从 768 维降到 8 维\npca = PCA(n_components=8)\nexplanation_embeddings_8d = pca.fit_transform(explanation_embeddings)  # shape: (200, 8)\n\n# STEP 4: Word2Vec 和数值特征的序列化\ndef row_to_sequence(row):\n    seq = []\n    for col in categorical_cols:\n        word = str(row[col])\n        vec = w2v_models[col].wv[word]\n        seq.append(vec)\n    for col in numerical_cols:\n        vec = np.full((embedding_size,), row[col])\n        seq.append(vec)\n    return np.stack(seq)  # shape: (len(categorical_cols + numerical_cols), embedding_size)\n\nX_seq_with_exp = []\n\nfor i in range(len(df)):\n    original_seq = row_to_sequence(df.iloc[i])  # shape: (seq_len, 8)\n    exp_vec_8d = explanation_embeddings_8d[i].reshape(1, -1)  # shape: (1, 8)\n    full_seq = np.concatenate([original_seq, exp_vec_8d], axis=0)  # shape: (seq_len + 1, 8) # 注意此处是直接进行拼接\n    X_seq_with_exp.append(full_seq)\n\nX_seq_with_exp = np.stack(X_seq_with_exp)\ny = df[target_col].values\n\n# STEP 5: 划分训练集\nX_train, X_test, y_train, y_test = train_test_split(X_seq_with_exp, y, test_size=0.3, random_state=42\n                                            #       , stratify=y\n                                                   )\n\n# STEP 6: 新的 Dataset 和 Dataloader\nclass SequenceDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self): return len(self.X)\n    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n\ntrain_loader = DataLoader(SequenceDataset(X_train, y_train), batch_size=16, shuffle=True)\ntest_loader = DataLoader(SequenceDataset(X_test, y_test), batch_size=16)\n\n# # ========== STEP 7: CNN ==========\n# class CNNWithBert(nn.Module):\n#     def __init__(self, input_dim=8, exp_dim=8, hidden_dim=32, output_dim=2, kernel_size=3):\n#         super().__init__()\n#         self.conv1d = nn.Conv1d(in_channels=input_dim, out_channels=hidden_dim, kernel_size=kernel_size, padding=1)\n#         self.pool = nn.AdaptiveMaxPool1d(1)  # 全局最大池化（获得 seq 表示）\n#         self.linear_exp = nn.Linear(exp_dim, hidden_dim)\n#         self.fc = nn.Linear(hidden_dim * 2, output_dim)\n\n#     def forward(self, x):\n#         x_cnn = x[:, :-1, :]     # shape: (batch, seq_len, input_dim)\n#         x_exp = x[:, -1, :]      # shape: (batch, exp_dim)\n\n#         # Conv1d expects (batch, channels=input_dim, seq_len)\n#         x_cnn = x_cnn.permute(0, 2, 1)     # (batch, input_dim, seq_len)\n#         cnn_feat = self.conv1d(x_cnn)      # (batch, hidden_dim, seq_len)\n#         cnn_feat = self.pool(cnn_feat)     # (batch, hidden_dim, 1)\n#         cnn_feat = cnn_feat.squeeze(-1)    # (batch, hidden_dim)\n\n#         exp_feat = self.linear_exp(x_exp)  # (batch, hidden_dim)\n\n#         out = torch.cat([cnn_feat, exp_feat], dim=1)  # (batch, hidden_dim * 2)\n#         return self.fc(out)\n\n# # 创建模型、损失函数和优化器\n# model = CNNWithBert(input_dim=8, exp_dim=8)\n\n# criterion = nn.CrossEntropyLoss()\n# optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n\nimport torch.nn as nn\n\nclass BiLSTMWithBert(nn.Module):\n    def __init__(self, input_dim=8, exp_dim=8, hidden_dim=32, output_dim=2, num_layers=1):\n        super().__init__()\n        self.bilstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=True\n        )\n        self.linear_exp = nn.Linear(exp_dim, hidden_dim * 2)\n        self.fc = nn.Linear(hidden_dim * 4, output_dim)\n\n    def forward(self, x):\n        x_seq = x[:, :-1, :]   # (batch, seq_len, input_dim)\n        x_exp = x[:, -1, :]    # (batch, exp_dim)\n\n        # BiLSTM outputs\n        lstm_out, _ = self.bilstm(x_seq)      # (batch, seq_len, hidden_dim*2)\n        lstm_feat = lstm_out.mean(dim=1)      # Mean pooling over time\n\n        exp_feat = self.linear_exp(x_exp)     # (batch, hidden_dim*2)\n\n        combined = torch.cat([lstm_feat, exp_feat], dim=1)  # (batch, hidden_dim*4)\n        return self.fc(combined)\n\nmodel = BiLSTMWithBert(input_dim=8, exp_dim=8, hidden_dim=32, output_dim=2)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n\n\n# STEP 8: 训练\nfor epoch in range(30):\n    model.train()\n    total_loss = 0\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        logits = model(X_batch)\n        loss = criterion(logits, y_batch)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n\n# STEP 9: 评估\nimport torch.nn.functional as F\n\nmodel.eval()\ny_true, y_pred, y_score = [], [], []\n\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        logits = model(X_batch)\n\n        # 获取预测标签（0或1）\n        preds = torch.argmax(logits, dim=1)\n        y_pred.extend(preds.cpu().numpy())\n\n        # 获取正类的概率值，假设是二分类，正类索引为 1\n        probs = F.softmax(logits, dim=1)[:, 1]\n        y_score.extend(probs.cpu().numpy())\n\n        y_true.extend(y_batch.cpu().numpy())\n\n\nfrom sklearn.metrics import (\n    precision_score, recall_score, f1_score, accuracy_score,\n    confusion_matrix, roc_auc_score\n)\n\ndef evaluate(y_true, y_pred, y_score=None):\n    precision = precision_score(y_true, y_pred, zero_division=0)\n    recall = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    acc = accuracy_score(y_true, y_pred)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    fpr = fp / (fp + tn + 1e-8)\n    pnr = fn / (fn + tp + 1e-8)\n    auc = roc_auc_score(y_true, y_score) if y_score is not None else None\n    return precision, recall, f1, acc, fpr, pnr, auc\n\n# 示例调用\nmetrics = evaluate(y_true, y_pred, y_score)\nnames = ['Precision', 'Recall', 'F1', 'Accuracy', 'FPR', 'PNR', 'AUC']\nfor n, v in zip(names, metrics):\n    print(f\"{n}: {v:.4f}\" if v is not None else f\"{n}: N/A\")\n\n# 训练直至收敛","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T04:49:17.991987Z","iopub.execute_input":"2025-07-17T04:49:17.992306Z","iopub.status.idle":"2025-07-17T04:49:46.549374Z","shell.execute_reply.started":"2025-07-17T04:49:17.992282Z","shell.execute_reply":"2025-07-17T04:49:46.548596Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 200/200 [00:27<00:00,  7.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 6.3723\nEpoch 2, Loss: 6.1931\nEpoch 3, Loss: 6.1062\nEpoch 4, Loss: 6.0105\nEpoch 5, Loss: 5.9272\nEpoch 6, Loss: 5.8737\nEpoch 7, Loss: 5.8073\nEpoch 8, Loss: 5.7663\nEpoch 9, Loss: 5.7127\nEpoch 10, Loss: 5.6629\nEpoch 11, Loss: 5.6194\nEpoch 12, Loss: 5.5539\nEpoch 13, Loss: 5.4850\nEpoch 14, Loss: 5.4197\nEpoch 15, Loss: 5.2894\nEpoch 16, Loss: 5.1900\nEpoch 17, Loss: 5.1722\nEpoch 18, Loss: 5.0582\nEpoch 19, Loss: 5.0024\nEpoch 20, Loss: 5.0399\nEpoch 21, Loss: 5.0726\nEpoch 22, Loss: 5.0131\nEpoch 23, Loss: 5.0537\nEpoch 24, Loss: 4.9804\nEpoch 25, Loss: 4.9902\nEpoch 26, Loss: 5.0122\nEpoch 27, Loss: 5.0305\nEpoch 28, Loss: 4.9951\nEpoch 29, Loss: 5.0436\nEpoch 30, Loss: 4.9920\nPrecision: 0.7143\nRecall: 0.5172\nF1: 0.6000\nAccuracy: 0.6667\nFPR: 0.1935\nPNR: 0.4828\nAUC: 0.7152\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# 3. Theory-LLM","metadata":{}},{"cell_type":"code","source":"# STEP 0: 引入 BERT 模块和 PCA\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.decomposition import PCA\nimport torch\nimport numpy as np\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\nfrom tqdm import tqdm\n\n# STEP 1: 加载 BERT 模型（一次性加载）\nbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nbert_model = BertModel.from_pretrained(\"bert-base-uncased\")\nbert_model.eval()  # 不训练\n\n# STEP 2: 获取 Explanation 的 BERT 向量\n@torch.no_grad()\ndef get_bert_embedding(text):\n    inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    outputs = bert_model(**inputs)\n    # 使用 [CLS] 向量作为整句表示\n    cls_embedding = outputs.last_hidden_state[:, 0, :]  # shape: [1, 768]\n    return cls_embedding.squeeze().numpy()  # shape: [768,]\n\n# 生成所有 Explanation 的 BERT 向量\nexplanation_embeddings = np.stack([get_bert_embedding(str(x)) for x in tqdm(df['all_Explanation'])])\n\n# STEP 3: 使用 PCA 降维将 BERT 向量从 768 维降到 8 维\npca = PCA(n_components=8)\nexplanation_embeddings_8d = pca.fit_transform(explanation_embeddings)  # shape: (200, 8)\n\n# STEP 4: Word2Vec 和数值特征的序列化\ndef row_to_sequence(row):\n    seq = []\n    for col in categorical_cols:\n        word = str(row[col])\n        vec = w2v_models[col].wv[word]\n        seq.append(vec)\n    for col in numerical_cols:\n        vec = np.full((embedding_size,), row[col])\n        seq.append(vec)\n    return np.stack(seq)  # shape: (len(categorical_cols + numerical_cols), embedding_size)\n\nX_seq_with_exp = []\n\nfor i in range(len(df)):\n    original_seq = row_to_sequence(df.iloc[i])  # shape: (seq_len, 8)\n    exp_vec_8d = explanation_embeddings_8d[i].reshape(1, -1)  # shape: (1, 8)\n    full_seq = np.concatenate([original_seq, exp_vec_8d], axis=0)  # shape: (seq_len + 1, 8) # 注意此处是直接进行拼接\n    X_seq_with_exp.append(full_seq)\n\nX_seq_with_exp = np.stack(X_seq_with_exp)\ny = df[target_col].values\n\n# STEP 5: 划分训练集\nX_train, X_test, y_train, y_test = train_test_split(X_seq_with_exp, y, test_size=0.3, random_state=42\n                                            #       , stratify=y\n                                                   )\n\n# STEP 6: 新的 Dataset 和 Dataloader\nclass SequenceDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self): return len(self.X)\n    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n\ntrain_loader = DataLoader(SequenceDataset(X_train, y_train), batch_size=16, shuffle=True)\ntest_loader = DataLoader(SequenceDataset(X_test, y_test), batch_size=16)\n\n# # ========== STEP 7: BiLSTM ==========\n\nimport torch.nn as nn\n\nclass BiLSTMWithBert(nn.Module):\n    def __init__(self, input_dim=8, exp_dim=8, hidden_dim=32, output_dim=2, num_layers=1):\n        super().__init__()\n        self.bilstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=True\n        )\n        self.linear_exp = nn.Linear(exp_dim, hidden_dim * 2)\n        self.fc = nn.Linear(hidden_dim * 4, output_dim)\n\n    def forward(self, x):\n        x_seq = x[:, :-1, :]   # (batch, seq_len, input_dim)\n        x_exp = x[:, -1, :]    # (batch, exp_dim)\n\n        # BiLSTM outputs\n        lstm_out, _ = self.bilstm(x_seq)      # (batch, seq_len, hidden_dim*2)\n        lstm_feat = lstm_out.mean(dim=1)      # Mean pooling over time\n\n        exp_feat = self.linear_exp(x_exp)     # (batch, hidden_dim*2)\n\n        combined = torch.cat([lstm_feat, exp_feat], dim=1)  # (batch, hidden_dim*4)\n        return self.fc(combined)\n\nmodel = BiLSTMWithBert(input_dim=8, exp_dim=8, hidden_dim=32, output_dim=2)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n\n\n# STEP 8: 训练\nfor epoch in range(30):\n    model.train()\n    total_loss = 0\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        logits = model(X_batch)\n        loss = criterion(logits, y_batch)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n\n# STEP 9: 评估\nimport torch.nn.functional as F\n\nmodel.eval()\ny_true, y_pred, y_score = [], [], []\n\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        logits = model(X_batch)\n\n        # 获取预测标签（0或1）\n        preds = torch.argmax(logits, dim=1)\n        y_pred.extend(preds.cpu().numpy())\n\n        # 获取正类的概率值，假设是二分类，正类索引为 1\n        probs = F.softmax(logits, dim=1)[:, 1]\n        y_score.extend(probs.cpu().numpy())\n\n        y_true.extend(y_batch.cpu().numpy())\n\n\nfrom sklearn.metrics import (\n    precision_score, recall_score, f1_score, accuracy_score,\n    confusion_matrix, roc_auc_score\n)\n\ndef evaluate(y_true, y_pred, y_score=None):\n    precision = precision_score(y_true, y_pred, zero_division=0)\n    recall = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    acc = accuracy_score(y_true, y_pred)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    fpr = fp / (fp + tn + 1e-8)\n    pnr = fn / (fn + tp + 1e-8)\n    auc = roc_auc_score(y_true, y_score) if y_score is not None else None\n    return precision, recall, f1, acc, fpr, pnr, auc\n\n# 示例调用\nmetrics = evaluate(y_true, y_pred, y_score)\nnames = ['Precision', 'Recall', 'F1', 'Accuracy', 'FPR', 'PNR', 'AUC']\nfor n, v in zip(names, metrics):\n    print(f\"{n}: {v:.4f}\" if v is not None else f\"{n}: N/A\")\n\n# 训练直至收敛","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T04:50:43.891024Z","iopub.execute_input":"2025-07-17T04:50:43.891343Z","iopub.status.idle":"2025-07-17T04:51:13.427031Z","shell.execute_reply.started":"2025-07-17T04:50:43.891318Z","shell.execute_reply":"2025-07-17T04:51:13.426260Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 200/200 [00:28<00:00,  7.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 6.4427\nEpoch 2, Loss: 6.1730\nEpoch 3, Loss: 5.9647\nEpoch 4, Loss: 5.8379\nEpoch 5, Loss: 5.7319\nEpoch 6, Loss: 5.6140\nEpoch 7, Loss: 5.5968\nEpoch 8, Loss: 5.5227\nEpoch 9, Loss: 5.4377\nEpoch 10, Loss: 5.4709\nEpoch 11, Loss: 5.4099\nEpoch 12, Loss: 5.3565\nEpoch 13, Loss: 5.3270\nEpoch 14, Loss: 5.2714\nEpoch 15, Loss: 5.2379\nEpoch 16, Loss: 5.1628\nEpoch 17, Loss: 5.0691\nEpoch 18, Loss: 4.9909\nEpoch 19, Loss: 4.9556\nEpoch 20, Loss: 4.9201\nEpoch 21, Loss: 4.9979\nEpoch 22, Loss: 4.9243\nEpoch 23, Loss: 4.9052\nEpoch 24, Loss: 4.9136\nEpoch 25, Loss: 4.8746\nEpoch 26, Loss: 4.8948\nEpoch 27, Loss: 4.8908\nEpoch 28, Loss: 4.8329\nEpoch 29, Loss: 4.8241\nEpoch 30, Loss: 4.9169\nPrecision: 0.7200\nRecall: 0.6207\nF1: 0.6667\nAccuracy: 0.7000\nFPR: 0.2258\nPNR: 0.3793\nAUC: 0.7442\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# 4. 加入Cross-attention","metadata":{}},{"cell_type":"code","source":"# STEP 0: 引入 BERT 模块和 PCA\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.decomposition import PCA\nimport torch\nimport numpy as np\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\nfrom tqdm import tqdm\n\n# STEP 1: 加载 BERT 模型（一次性加载）\nbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nbert_model = BertModel.from_pretrained(\"bert-base-uncased\")\nbert_model.eval()  # 不训练\n\n# STEP 2: 获取 Explanation 的 BERT 向量\n@torch.no_grad()\ndef get_bert_embedding(text):\n    inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    outputs = bert_model(**inputs)\n    # 使用 [CLS] 向量作为整句表示\n    cls_embedding = outputs.last_hidden_state[:, 0, :]  # shape: [1, 768]\n    return cls_embedding.squeeze().numpy()  # shape: [768,]\n\n# 生成所有 Explanation 的 BERT 向量\nexplanation_embeddings = np.stack([get_bert_embedding(str(x)) for x in tqdm(df['all_Explanation'])])\n\n# STEP 3: 使用 PCA 降维将 BERT 向量从 768 维降到 8 维\npca = PCA(n_components=8)\nexplanation_embeddings_8d = pca.fit_transform(explanation_embeddings)  # shape: (200, 8)\n\n# STEP 4: Word2Vec 和数值特征的序列化\ndef row_to_sequence(row):\n    seq = []\n    for col in categorical_cols:\n        word = str(row[col])\n        vec = w2v_models[col].wv[word]\n        seq.append(vec)\n    for col in numerical_cols:\n        vec = np.full((embedding_size,), row[col])\n        seq.append(vec)\n    return np.stack(seq)  # shape: (len(categorical_cols + numerical_cols), embedding_size)\n\n# 选项1 直接拼接\n\nX_seq_with_exp = []\n\nfor i in range(len(df)):\n    original_seq = row_to_sequence(df.iloc[i])  # shape: (seq_len, 8)\n    exp_vec_8d = explanation_embeddings_8d[i].reshape(1, -1)  # shape: (1, 8)\n    full_seq = np.concatenate([original_seq, exp_vec_8d], axis=0)  # shape: (seq_len + 1, 8) # 注意此处是直接进行拼接\n    X_seq_with_exp.append(full_seq)\n\nX_seq_with_exp = np.stack(X_seq_with_exp)\ny = df[target_col].values\n\n# # 选项2 使用 Cross-attention 融合\n# import torch\n# import torch.nn as nn\n\n# # 定义 Cross-Attention 模块\n# class CrossAttentionFusion(nn.Module):\n#     def __init__(self, embed_dim=8, num_heads=1):\n#         super(CrossAttentionFusion, self).__init__()\n#         self.cross_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n#         self.norm = nn.LayerNorm(embed_dim)\n\n#     def forward(self, query_vec, context_seq):\n#         # query_vec: (B, 1, E)\n#         # context_seq: (B, L, E)\n#         attn_output, _ = self.cross_attn(query_vec, context_seq, context_seq)\n# #        attn_output, _ = self.cross_attn(context_seq, query_vec, query_vec) 效果不好\n#         fused = self.norm(query_vec + attn_output)\n#         return fused  # shape: (B, 1, E)\n\n# # 初始化模块（可以放在外部）\n# cross_attention = CrossAttentionFusion(embed_dim=8, num_heads=1)\n\n# # 融合序列和解释向量\n# X_seq_with_exp = []\n\n# for i in range(len(df)):\n#     row = df.iloc[i]\n    \n#     original_seq = row_to_sequence(row)  # shape: (seq_len, 8)\n#     exp_vec_8d = explanation_embeddings_8d[i].reshape(1, 8)  # shape: (1, 8)\n\n#     # 转换为 tensor\n#     context_seq = torch.tensor(original_seq, dtype=torch.float32).unsqueeze(0)  # shape: (1, seq_len, 8)\n#     query_vec = torch.tensor(exp_vec_8d, dtype=torch.float32).unsqueeze(0)      # shape: (1, 1, 8)\n\n#     # 运行 cross-attention\n#     fused_vec = cross_attention(query_vec, context_seq)  # shape: (1, 1, 8)\n#     fused_vec_np = fused_vec.squeeze(0).detach().numpy()  # shape: (1, 8)\n\n#     # 选择如何使用融合后的向量：拼接 or 替换\n#     full_seq = np.concatenate([original_seq, fused_vec_np], axis=0)  # shape: (seq_len + 1, 8)\n\n#     X_seq_with_exp.append(full_seq)\n\n# X_seq_with_exp = np.stack(X_seq_with_exp)  # shape: (num_samples, seq_len + 1, 8)\n# y = df[target_col].values\n\n\n# STEP 5: 划分训练集\nX_train, X_test, y_train, y_test = train_test_split(X_seq_with_exp, y, test_size=0.3, random_state=42\n                                            #       , stratify=y\n                                                   )\n\n# STEP 6: 新的 Dataset 和 Dataloader\nclass SequenceDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self): return len(self.X)\n    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n\ntrain_loader = DataLoader(SequenceDataset(X_train, y_train), batch_size=16, shuffle=True)\ntest_loader = DataLoader(SequenceDataset(X_test, y_test), batch_size=16)\n\n# # ========== STEP 7: BiLSTM ==========\n\nimport torch.nn as nn\n\nclass BiLSTMWithBert(nn.Module):\n    def __init__(self, input_dim=8, exp_dim=8, hidden_dim=32, output_dim=2, num_layers=1):\n        super().__init__()\n        self.bilstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=True\n        )\n        self.linear_exp = nn.Linear(exp_dim, hidden_dim * 2)\n        self.fc = nn.Linear(hidden_dim * 4, output_dim)\n\n    def forward(self, x):\n        x_seq = x[:, :-1, :]   # (batch, seq_len, input_dim)\n        x_exp = x[:, -1, :]    # (batch, exp_dim)\n\n        # BiLSTM outputs\n        lstm_out, _ = self.bilstm(x_seq)      # (batch, seq_len, hidden_dim*2)\n        lstm_feat = lstm_out.mean(dim=1)      # Mean pooling over time\n\n        exp_feat = self.linear_exp(x_exp)     # (batch, hidden_dim*2)\n\n        combined = torch.cat([lstm_feat, exp_feat], dim=1)  # (batch, hidden_dim*4)\n        return self.fc(combined)\n\nmodel = BiLSTMWithBert(input_dim=8, exp_dim=8, hidden_dim=32, output_dim=2)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n\n\n# STEP 8: 训练\nfor epoch in range(30):\n    model.train()\n    total_loss = 0\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        logits = model(X_batch)\n        loss = criterion(logits, y_batch)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n\n# STEP 9: 评估\nimport torch.nn.functional as F\n\nmodel.eval()\ny_true, y_pred, y_score = [], [], []\n\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        logits = model(X_batch)\n\n        # 获取预测标签（0或1）\n        preds = torch.argmax(logits, dim=1)\n        y_pred.extend(preds.cpu().numpy())\n\n        # 获取正类的概率值，假设是二分类，正类索引为 1\n        probs = F.softmax(logits, dim=1)[:, 1]\n        y_score.extend(probs.cpu().numpy())\n\n        y_true.extend(y_batch.cpu().numpy())\n\n\nfrom sklearn.metrics import (\n    precision_score, recall_score, f1_score, accuracy_score,\n    confusion_matrix, roc_auc_score\n)\n\ndef evaluate(y_true, y_pred, y_score=None):\n    precision = precision_score(y_true, y_pred, zero_division=0)\n    recall = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    acc = accuracy_score(y_true, y_pred)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    fpr = fp / (fp + tn + 1e-8)\n    pnr = fn / (fn + tp + 1e-8)\n    auc = roc_auc_score(y_true, y_score) if y_score is not None else None\n    return precision, recall, f1, acc, fpr, pnr, auc\n\n# 示例调用\nmetrics = evaluate(y_true, y_pred, y_score)\nnames = ['Precision', 'Recall', 'F1', 'Accuracy', 'FPR', 'PNR', 'AUC']\nfor n, v in zip(names, metrics):\n    print(f\"{n}: {v:.4f}\" if v is not None else f\"{n}: N/A\")\n\n# 训练直至收敛","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:42:25.755262Z","iopub.execute_input":"2025-07-17T05:42:25.755550Z","iopub.status.idle":"2025-07-17T05:42:54.866203Z","shell.execute_reply.started":"2025-07-17T05:42:25.755530Z","shell.execute_reply":"2025-07-17T05:42:54.865475Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 200/200 [00:27<00:00,  7.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 5.8144\nEpoch 2, Loss: 5.6707\nEpoch 3, Loss: 5.6062\nEpoch 4, Loss: 5.5370\nEpoch 5, Loss: 5.4489\nEpoch 6, Loss: 5.4045\nEpoch 7, Loss: 5.4032\nEpoch 8, Loss: 5.3559\nEpoch 9, Loss: 5.3402\nEpoch 10, Loss: 5.3096\nEpoch 11, Loss: 5.2813\nEpoch 12, Loss: 5.2817\nEpoch 13, Loss: 5.2324\nEpoch 14, Loss: 5.2128\nEpoch 15, Loss: 5.0983\nEpoch 16, Loss: 5.0778\nEpoch 17, Loss: 4.9486\nEpoch 18, Loss: 4.9677\nEpoch 19, Loss: 4.8982\nEpoch 20, Loss: 4.9008\nEpoch 21, Loss: 4.8217\nEpoch 22, Loss: 4.8595\nEpoch 23, Loss: 4.8849\nEpoch 24, Loss: 4.8649\nEpoch 25, Loss: 4.8343\nEpoch 26, Loss: 4.8430\nEpoch 27, Loss: 4.8039\nEpoch 28, Loss: 4.8092\nEpoch 29, Loss: 4.9108\nEpoch 30, Loss: 4.8071\nPrecision: 0.6923\nRecall: 0.6207\nF1: 0.6545\nAccuracy: 0.6833\nFPR: 0.2581\nPNR: 0.3793\nAUC: 0.7397\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# 5. 只加入对比学习 【精进】","metadata":{}},{"cell_type":"code","source":"# STEP 0: 引入 BERT 模块和 PCA\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.decomposition import PCA\nimport torch\nimport numpy as np\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\nfrom tqdm import tqdm\n\n# STEP 1: 加载 BERT 模型（一次性加载）\nbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nbert_model = BertModel.from_pretrained(\"bert-base-uncased\")\nbert_model.eval()  # 不训练\n\n# STEP 2: 获取 Explanation 的 BERT 向量\n@torch.no_grad()\ndef get_bert_embedding(text):\n    inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    outputs = bert_model(**inputs)\n    # 使用 [CLS] 向量作为整句表示\n    cls_embedding = outputs.last_hidden_state[:, 0, :]  # shape: [1, 768]\n    return cls_embedding.squeeze().numpy()  # shape: [768,]\n\n# 生成所有 Explanation 的 BERT 向量\nexplanation_embeddings = np.stack([get_bert_embedding(str(x)) for x in tqdm(df['all_Explanation'])])\n\n# STEP 3: 使用 PCA 降维将 BERT 向量从 768 维降到 8 维\npca = PCA(n_components=8)\nexplanation_embeddings_8d = pca.fit_transform(explanation_embeddings)  # shape: (200, 8)\n\n# STEP 4: Word2Vec 和数值特征的序列化\ndef row_to_sequence(row):\n    seq = []\n    for col in categorical_cols:\n        word = str(row[col])\n        vec = w2v_models[col].wv[word]\n        seq.append(vec)\n    for col in numerical_cols:\n        vec = np.full((embedding_size,), row[col])\n        seq.append(vec)\n    return np.stack(seq)  # shape: (len(categorical_cols + numerical_cols), embedding_size)\n\n# 直接拼接\n\nX_seq_with_exp = []\n\nfor i in range(len(df)):\n    original_seq = row_to_sequence(df.iloc[i])  # shape: (seq_len, 8)\n    exp_vec_8d = explanation_embeddings_8d[i].reshape(1, -1)  # shape: (1, 8)\n    full_seq = np.concatenate([original_seq, exp_vec_8d], axis=0)  # shape: (seq_len + 1, 8) # 注意此处是直接进行拼接\n    X_seq_with_exp.append(full_seq)\n\nX_seq_with_exp = np.stack(X_seq_with_exp) # 注意， 此时的input已经为LLM-enhanced embedding \ny = df[target_col].values\n\n\n# STEP 5: 划分训练集\nX_train, X_test, y_train, y_test = train_test_split(X_seq_with_exp, y, test_size=0.3, random_state=42\n                                            #       , stratify=y\n                                                   )\n\n# STEP 6: 新的 Dataset 和 Dataloader\nclass SequenceDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self): return len(self.X)\n    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n\ntrain_loader = DataLoader(SequenceDataset(X_train, y_train), batch_size=16, shuffle=True)\ntest_loader = DataLoader(SequenceDataset(X_test, y_test), batch_size=16)\n\n# # ========== STEP 7: BiLSTM ==========\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BiLSTMWithBert(nn.Module):\n    def __init__(self, input_dim=8, exp_dim=8, hidden_dim=32, output_dim=2, num_layers=1):\n        super().__init__()\n        self.bilstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=True\n        )\n        self.linear_exp = nn.Linear(exp_dim, hidden_dim * 2)\n        self.fc = nn.Linear(hidden_dim * 4, output_dim)\n\n    def forward(self, x):\n        x_seq = x[:, :-1, :]   # (batch, seq_len, input_dim)\n        x_exp = x[:, -1, :]    # (batch, exp_dim)\n\n        lstm_out, _ = self.bilstm(x_seq)\n        lstm_feat = lstm_out.mean(dim=1)              # (batch, hidden_dim*2)\n        exp_feat = self.linear_exp(x_exp)             # (batch, hidden_dim*2)\n\n        combined = torch.cat([lstm_feat, exp_feat], dim=1)  # (batch, hidden_dim*4)\n        logits = self.fc(combined)\n\n        return logits, lstm_feat, exp_feat  # 输出三个部分：分类、原始特征、解释特征\n\n# 设计对比学习的loss\nclass NTXentLoss(nn.Module):\n    def __init__(self, temperature=0.5):\n        super().__init__()\n        self.temperature = temperature\n\n    def forward(self, z1, z2):\n        z1 = F.normalize(z1, dim=1)\n        z2 = F.normalize(z2, dim=1)\n        batch_size = z1.size(0)\n\n        representations = torch.cat([z1, z2], dim=0)  # [2B, D]\n        similarity_matrix = torch.matmul(representations, representations.T)  # [2B, 2B]\n\n        labels = torch.arange(batch_size).to(z1.device)\n        labels = torch.cat([labels, labels], dim=0)\n\n        # Remove self-similarity\n        mask = torch.eye(2 * batch_size, dtype=torch.bool).to(z1.device)\n        similarity_matrix = similarity_matrix.masked_fill(mask, -9e15)\n\n        similarity_matrix = similarity_matrix / self.temperature\n\n        loss = F.cross_entropy(similarity_matrix, labels)\n        return loss\n        \n# 训练\nmodel = BiLSTMWithBert(input_dim=8, exp_dim=8, hidden_dim=32, output_dim=2)\nce_criterion = nn.CrossEntropyLoss()\ncontrastive_criterion = NTXentLoss(temperature=0.5)\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n\nfor epoch in range(30):\n    model.train()\n    total_loss = 0\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        logits, lstm_feat, exp_feat = model(X_batch)\n\n        # Cross-entropy loss\n        ce_loss = ce_criterion(logits, y_batch)\n\n        # Contrastive loss between original and explanation embeddings\n        contrastive_loss = contrastive_criterion(lstm_feat, exp_feat)\n\n        loss = ce_loss + 0.05 * contrastive_loss  # 权重可以调整（建议范围：0.05 - 0.2）\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n\n\n# # STEP 9: 评估\n\nmodel.eval()\ny_true, y_pred, y_score = [], [], []\n\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        logits, _, _ = model(X_batch)\n\n        # 获取预测标签（0或1）\n        preds = torch.argmax(logits, dim=1)\n        y_pred.extend(preds.cpu().numpy())\n\n        # 获取正类的概率值\n        probs = F.softmax(logits, dim=1)[:, 1]\n        y_score.extend(probs.cpu().numpy())\n\n        y_true.extend(y_batch.cpu().numpy())\n\n\nfrom sklearn.metrics import (\n    precision_score, recall_score, f1_score, accuracy_score,\n    confusion_matrix, roc_auc_score\n)\n\ndef evaluate(y_true, y_pred, y_score=None):\n    precision = precision_score(y_true, y_pred, zero_division=0)\n    recall = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    acc = accuracy_score(y_true, y_pred)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    fpr = fp / (fp + tn + 1e-8)\n    pnr = fn / (fn + tp + 1e-8)\n    auc = roc_auc_score(y_true, y_score) if y_score is not None else None\n    return precision, recall, f1, acc, fpr, pnr, auc\n\n# 示例调用\nmetrics = evaluate(y_true, y_pred, y_score)\nnames = ['Precision', 'Recall', 'F1', 'Accuracy', 'FPR', 'PNR', 'AUC']\nfor n, v in zip(names, metrics):\n    print(f\"{n}: {v:.4f}\" if v is not None else f\"{n}: N/A\")\n\n# 训练直至收敛","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T06:11:53.556421Z","iopub.execute_input":"2025-07-17T06:11:53.556991Z","iopub.status.idle":"2025-07-17T06:12:22.106374Z","shell.execute_reply.started":"2025-07-17T06:11:53.556967Z","shell.execute_reply":"2025-07-17T06:12:22.105697Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 200/200 [00:26<00:00,  7.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 4050000279502848.0000\nEpoch 2, Loss: 4050000279502848.0000\nEpoch 3, Loss: 4050000279502848.0000\nEpoch 4, Loss: 4050000279502848.0000\nEpoch 5, Loss: 4050000279502848.0000\nEpoch 6, Loss: 4050000279502848.0000\nEpoch 7, Loss: 4050000279502848.0000\nEpoch 8, Loss: 4050000279502848.0000\nEpoch 9, Loss: 4050000279502848.0000\nEpoch 10, Loss: 4050000279502848.0000\nEpoch 11, Loss: 4050000279502848.0000\nEpoch 12, Loss: 4050000279502848.0000\nEpoch 13, Loss: 4050000279502848.0000\nEpoch 14, Loss: 4050000279502848.0000\nEpoch 15, Loss: 4050000279502848.0000\nEpoch 16, Loss: 4050000279502848.0000\nEpoch 17, Loss: 4050000279502848.0000\nEpoch 18, Loss: 4050000279502848.0000\nEpoch 19, Loss: 4050000279502848.0000\nEpoch 20, Loss: 4050000279502848.0000\nEpoch 21, Loss: 4050000279502848.0000\nEpoch 22, Loss: 4050000279502848.0000\nEpoch 23, Loss: 4050000279502848.0000\nEpoch 24, Loss: 4050000279502848.0000\nEpoch 25, Loss: 4050000279502848.0000\nEpoch 26, Loss: 4050000279502848.0000\nEpoch 27, Loss: 4050000279502848.0000\nEpoch 28, Loss: 4050000279502848.0000\nEpoch 29, Loss: 4050000279502848.0000\nEpoch 30, Loss: 4050000279502848.0000\nPrecision: 0.7037\nRecall: 0.6552\nF1: 0.6786\nAccuracy: 0.7000\nFPR: 0.2581\nPNR: 0.3448\nAUC: 0.7464\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# 5.1 两个角度精进对比学习","metadata":{}},{"cell_type":"code","source":"# STEP 0: 引入 BERT 模块和 PCA\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.decomposition import PCA\nimport torch\nimport numpy as np\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\nfrom tqdm import tqdm\n\n# STEP 1: 加载 BERT 模型（一次性加载）\nbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nbert_model = BertModel.from_pretrained(\"bert-base-uncased\")\nbert_model.eval()  # 不训练\n\n# STEP 2: 获取 Explanation 的 BERT 向量\n@torch.no_grad()\ndef get_bert_embedding(text):\n    inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    outputs = bert_model(**inputs)\n    # 使用 [CLS] 向量作为整句表示\n    cls_embedding = outputs.last_hidden_state[:, 0, :]  # shape: [1, 768]\n    return cls_embedding.squeeze().numpy()  # shape: [768,]\n\n# 生成所有 Explanation 的 BERT 向量\nexplanation_embeddings = np.stack([get_bert_embedding(str(x)) for x in tqdm(df['all_Explanation'])])\n\n# STEP 3: 使用 PCA 降维将 BERT 向量从 768 维降到 8 维\npca = PCA(n_components=8)\nexplanation_embeddings_8d = pca.fit_transform(explanation_embeddings)  # shape: (200, 8)\n\n# STEP 4: Word2Vec 和数值特征的序列化\ndef row_to_sequence(row):\n    seq = []\n    for col in categorical_cols:\n        word = str(row[col])\n        vec = w2v_models[col].wv[word]\n        seq.append(vec)\n    for col in numerical_cols:\n        vec = np.full((embedding_size,), row[col])\n        seq.append(vec)\n    return np.stack(seq)  # shape: (len(categorical_cols + numerical_cols), embedding_size)\n\nX_seq_with_exp = []\n\nfor i in range(len(df)):\n    original_seq = row_to_sequence(df.iloc[i])  # shape: (seq_len, 8)\n    exp_vec_8d = explanation_embeddings_8d[i].reshape(1, -1)  # shape: (1, 8)\n    full_seq = np.concatenate([original_seq, exp_vec_8d], axis=0)  # shape: (seq_len + 1, 8) # 注意此处是直接进行拼接\n    X_seq_with_exp.append(full_seq)\n\nX_seq_with_exp = np.stack(X_seq_with_exp)\ny = df[target_col].values\n\n# STEP 5: 划分训练集\nX_train, X_test, y_train, y_test = train_test_split(X_seq_with_exp, y, test_size=0.3, random_state=42\n                                            #       , stratify=y\n                                                   )\n\n# STEP 6: 新的 Dataset 和 Dataloader\nclass SequenceDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self): return len(self.X)\n    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n\ntrain_loader = DataLoader(SequenceDataset(X_train, y_train), batch_size=16, shuffle=True)\ntest_loader = DataLoader(SequenceDataset(X_test, y_test), batch_size=16)\n\n# # ========== STEP 7: BiLSTM ==========\n\nimport torch.nn as nn\n\nclass BiLSTMWithBert(nn.Module):\n    def __init__(self, input_dim=8, exp_dim=8, hidden_dim=32, output_dim=2, num_layers=1):\n        super().__init__()\n        self.bilstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=True\n        )\n        self.linear_exp = nn.Linear(exp_dim, hidden_dim * 2)\n        self.fc = nn.Linear(hidden_dim * 4, output_dim)\n\n    def forward(self, x):\n        x_seq = x[:, :-1, :]   # (batch, seq_len, input_dim)\n        x_exp = x[:, -1, :]    # (batch, exp_dim)\n\n        # BiLSTM outputs\n        lstm_out, _ = self.bilstm(x_seq)      # (batch, seq_len, hidden_dim*2)\n        lstm_feat = lstm_out.mean(dim=1)      # Mean pooling over time\n\n        exp_feat = self.linear_exp(x_exp)     # (batch, hidden_dim*2)\n\n        combined = torch.cat([lstm_feat, exp_feat], dim=1)  # (batch, hidden_dim*4)\n        return self.fc(combined)\n\nmodel = BiLSTMWithBert(input_dim=8, exp_dim=8, hidden_dim=32, output_dim=2)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n\n# 定义对比损失\nimport torch.nn.functional as F\n\ndef cosine_sim(a, b):\n    a_norm = F.normalize(a, p=2, dim=-1)\n    b_norm = F.normalize(b, p=2, dim=-1)\n    return torch.matmul(a_norm, b_norm.T)  # shape: (batch, batch)\n\ndef info_nce_loss(anchors, positives, temperature=0.5):\n    sim_matrix = cosine_sim(anchors, positives) / temperature\n    labels = torch.arange(sim_matrix.size(0)).to(sim_matrix.device)\n    return F.cross_entropy(sim_matrix, labels)\n\n\n# Step 8 模型训练\n\nfor epoch in range(30):\n    model.train()\n    total_loss = 0\n\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n\n        logits = model(X_batch)  # shape: (B, 2)\n        loss_cls = criterion(logits, y_batch)  # 分类损失\n\n        # =========== Contrastive Loss ============\n        # Step 1: 获取 BiLSTM 的 embedding 和 explanation 的 embedding\n        x_seq = X_batch[:, :-1, :]\n        x_exp = X_batch[:, -1, :]\n\n        with torch.no_grad():\n            lstm_out, _ = model.bilstm(x_seq)\n            lstm_feat = lstm_out.mean(dim=1)         # shape: (B, hidden*2)\n            exp_feat = model.linear_exp(x_exp)       # shape: (B, hidden*2)\n\n        # Step 2: 利用 cosine 相似度筛选出正负样本对\n        cos_sim_seq = cosine_sim(lstm_feat, lstm_feat)      # shape: (B, B)\n        cos_sim_exp = cosine_sim(exp_feat, exp_feat)        # shape: (B, B)\n\n        # Step 3: 构造 InfoNCE loss（正样本：两个角度都相似；负样本：都不相似）\n        # 注意：我们只选对角线（i==j）为正，其余为负（标准 InfoNCE 设定）\n        loss_contrast_seq = info_nce_loss(lstm_feat, lstm_feat)  # 原始视角\n        loss_contrast_exp = info_nce_loss(exp_feat, exp_feat)    # Explanation 视角\n\n        # Total Loss: 分类损失 + 对比损失（加权求和）\n        loss = loss_cls + 0.05 * (loss_contrast_seq + loss_contrast_exp)\n\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}, Total Loss: {total_loss:.4f}\")\n\n\n# STEP 9: 评估\nimport torch.nn.functional as F\n\nmodel.eval()\ny_true, y_pred, y_score = [], [], []\n\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        logits = model(X_batch)\n\n        # 获取预测标签（0或1）\n        preds = torch.argmax(logits, dim=1)\n        y_pred.extend(preds.cpu().numpy())\n\n        # 获取正类的概率值，假设是二分类，正类索引为 1\n        probs = F.softmax(logits, dim=1)[:, 1]\n        y_score.extend(probs.cpu().numpy())\n\n        y_true.extend(y_batch.cpu().numpy())\n\n\nfrom sklearn.metrics import (\n    precision_score, recall_score, f1_score, accuracy_score,\n    confusion_matrix, roc_auc_score\n)\n\ndef evaluate(y_true, y_pred, y_score=None):\n    precision = precision_score(y_true, y_pred, zero_division=0)\n    recall = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    acc = accuracy_score(y_true, y_pred)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    fpr = fp / (fp + tn + 1e-8)\n    pnr = fn / (fn + tp + 1e-8)\n    auc = roc_auc_score(y_true, y_score) if y_score is not None else None\n    return precision, recall, f1, acc, fpr, pnr, auc\n\n# 示例调用\nmetrics = evaluate(y_true, y_pred, y_score)\nnames = ['Precision', 'Recall', 'F1', 'Accuracy', 'FPR', 'PNR', 'AUC']\nfor n, v in zip(names, metrics):\n    print(f\"{n}: {v:.4f}\" if v is not None else f\"{n}: N/A\")\n\n# 训练直至收敛","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T07:50:09.125320Z","iopub.execute_input":"2025-07-17T07:50:09.125607Z","iopub.status.idle":"2025-07-17T07:50:40.067943Z","shell.execute_reply.started":"2025-07-17T07:50:09.125588Z","shell.execute_reply":"2025-07-17T07:50:40.067252Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 200/200 [00:29<00:00,  6.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Total Loss: 7.9657\nEpoch 2, Total Loss: 7.7175\nEpoch 3, Total Loss: 7.5762\nEpoch 4, Total Loss: 7.4171\nEpoch 5, Total Loss: 7.4271\nEpoch 6, Total Loss: 7.3677\nEpoch 7, Total Loss: 7.3353\nEpoch 8, Total Loss: 7.2802\nEpoch 9, Total Loss: 7.2447\nEpoch 10, Total Loss: 7.2024\nEpoch 11, Total Loss: 7.1632\nEpoch 12, Total Loss: 7.1222\nEpoch 13, Total Loss: 7.0546\nEpoch 14, Total Loss: 7.0140\nEpoch 15, Total Loss: 6.9564\nEpoch 16, Total Loss: 6.8298\nEpoch 17, Total Loss: 6.7001\nEpoch 18, Total Loss: 6.6655\nEpoch 19, Total Loss: 6.5782\nEpoch 20, Total Loss: 6.5961\nEpoch 21, Total Loss: 6.5205\nEpoch 22, Total Loss: 6.4468\nEpoch 23, Total Loss: 6.6056\nEpoch 24, Total Loss: 6.4756\nEpoch 25, Total Loss: 6.4491\nEpoch 26, Total Loss: 6.4558\nEpoch 27, Total Loss: 6.4730\nEpoch 28, Total Loss: 6.3982\nEpoch 29, Total Loss: 6.4388\nEpoch 30, Total Loss: 6.4977\nPrecision: 0.7200\nRecall: 0.6207\nF1: 0.6667\nAccuracy: 0.7000\nFPR: 0.2258\nPNR: 0.3793\nAUC: 0.7497\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"# 5.2 从一个角度精进对比学习","metadata":{}},{"cell_type":"code","source":"# STEP 0: 引入 BERT 模块和 PCA\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.decomposition import PCA\nimport torch\nimport numpy as np\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\nfrom tqdm import tqdm\n\n# STEP 1: 加载 BERT 模型（一次性加载）\nbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nbert_model = BertModel.from_pretrained(\"bert-base-uncased\")\nbert_model.eval()  # 不训练\n\n# STEP 2: 获取 Explanation 的 BERT 向量\n@torch.no_grad()\ndef get_bert_embedding(text):\n    inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    outputs = bert_model(**inputs)\n    # 使用 [CLS] 向量作为整句表示\n    cls_embedding = outputs.last_hidden_state[:, 0, :]  # shape: [1, 768]\n    return cls_embedding.squeeze().numpy()  # shape: [768,]\n\n# 生成所有 Explanation 的 BERT 向量\nexplanation_embeddings = np.stack([get_bert_embedding(str(x)) for x in tqdm(df['all_Explanation'])])\n\n# STEP 3: 使用 PCA 降维将 BERT 向量从 768 维降到 8 维\npca = PCA(n_components=8)\nexplanation_embeddings_8d = pca.fit_transform(explanation_embeddings)  # shape: (200, 8)\n\n# STEP 4: Word2Vec 和数值特征的序列化\ndef row_to_sequence(row):\n    seq = []\n    for col in categorical_cols:\n        word = str(row[col])\n        vec = w2v_models[col].wv[word]\n        seq.append(vec)\n    for col in numerical_cols:\n        vec = np.full((embedding_size,), row[col])\n        seq.append(vec)\n    return np.stack(seq)  # shape: (len(categorical_cols + numerical_cols), embedding_size)\n\nX_seq_with_exp = []\n\nfor i in range(len(df)):\n    original_seq = row_to_sequence(df.iloc[i])  # shape: (seq_len, 8)\n    exp_vec_8d = explanation_embeddings_8d[i].reshape(1, -1)  # shape: (1, 8)\n    full_seq = np.concatenate([original_seq, exp_vec_8d], axis=0)  # shape: (seq_len + 1, 8) # 注意此处是直接进行拼接\n    X_seq_with_exp.append(full_seq)\n\nX_seq_with_exp = np.stack(X_seq_with_exp)\ny = df[target_col].values\n\n# STEP 5: 划分训练集\nX_train, X_test, y_train, y_test = train_test_split(X_seq_with_exp, y, test_size=0.3, random_state=42\n                                            #       , stratify=y\n                                                   )\n\n# STEP 6: 新的 Dataset 和 Dataloader\nclass SequenceDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self): return len(self.X)\n    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n\ntrain_loader = DataLoader(SequenceDataset(X_train, y_train), batch_size=16, shuffle=True)\ntest_loader = DataLoader(SequenceDataset(X_test, y_test), batch_size=16)\n\n# # ========== STEP 7: BiLSTM ==========\n\nimport torch.nn as nn\n\nclass BiLSTMWithBert(nn.Module):\n    def __init__(self, input_dim=8, exp_dim=8, hidden_dim=32, output_dim=2, num_layers=1):\n        super().__init__()\n        self.bilstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=True\n        )\n        self.linear_exp = nn.Linear(exp_dim, hidden_dim * 2)\n        self.fc = nn.Linear(hidden_dim * 4, output_dim)\n\n    def forward(self, x):\n        x_seq = x[:, :-1, :]   # (batch, seq_len, input_dim)\n        x_exp = x[:, -1, :]    # (batch, exp_dim)\n\n        # BiLSTM outputs\n        lstm_out, _ = self.bilstm(x_seq)      # (batch, seq_len, hidden_dim*2)\n        lstm_feat = lstm_out.mean(dim=1)      # Mean pooling over time\n\n        exp_feat = self.linear_exp(x_exp)     # (batch, hidden_dim*2)\n\n        combined = torch.cat([lstm_feat, exp_feat], dim=1)  # (batch, hidden_dim*4)\n        return self.fc(combined)\n\nmodel = BiLSTMWithBert(input_dim=8, exp_dim=8, hidden_dim=32, output_dim=2)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n\nimport torch.nn.functional as F\n\ndef info_nce_loss(features, labels, temperature=0.5):\n    \"\"\"\n    features: tensor of shape (batch_size, embed_dim)\n    labels: tensor of shape (batch_size,) with class labels\n    \"\"\"\n    # Normalize features\n    features = F.normalize(features, dim=1)  # cosine sim\n    similarity_matrix = torch.matmul(features, features.T)  # (batch, batch)\n\n    # Build labels mask (positive pairs: same label)\n    labels = labels.contiguous().view(-1, 1)\n    mask = torch.eq(labels, labels.T).float().to(features.device)  # (batch, batch)\n\n    # Avoid self-similarity\n    self_mask = torch.eye(mask.shape[0], device=features.device)\n    mask = mask - self_mask  # 1 for positive pairs only, 0 elsewhere\n\n    # Scale similarity\n    logits = similarity_matrix / temperature\n\n    # For numerical stability\n    logits_max, _ = torch.max(logits, dim=1, keepdim=True)\n    logits = logits - logits_max.detach()\n\n    exp_logits = torch.exp(logits) * (1 - self_mask)\n    log_prob = logits - torch.log(exp_logits.sum(dim=1, keepdim=True) + 1e-8)\n\n    # Compute mean of log-likelihood over positive\n    mean_log_prob_pos = (mask * log_prob).sum(dim=1) / (mask.sum(dim=1) + 1e-8)\n\n    loss = -mean_log_prob_pos.mean()\n    return loss\n\nfor epoch in range(30):\n    model.train()\n    total_loss = 0\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        logits = model(X_batch)\n\n        # 原始分类损失\n        loss_cls = criterion(logits, y_batch)\n\n        # 提取 BiLSTM 和解释向量的中间表示\n        x_seq = X_batch[:, :-1, :]\n        x_exp = X_batch[:, -1, :]\n        \n        lstm_out, _ = model.bilstm(x_seq)\n        lstm_feat = lstm_out.mean(dim=1)  # (batch, hidden_dim*2)\n        exp_feat = model.linear_exp(x_exp)  # (batch, hidden_dim*2)\n        \n        combined_feat = torch.cat([lstm_feat, exp_feat], dim=1)  # (batch, hidden_dim*4)\n\n        # 对比损失\n        loss_contrast = info_nce_loss(combined_feat, y_batch)\n\n        # 总损失 = 分类 + 对比\n        loss = loss_cls + 0.05 * loss_contrast  # 超参数 可调整0.05为最佳\n\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n\n\n# STEP 9: 评估\nimport torch.nn.functional as F\n\nmodel.eval()\ny_true, y_pred, y_score = [], [], []\n\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        logits = model(X_batch)\n\n        # 获取预测标签（0或1）\n        preds = torch.argmax(logits, dim=1)\n        y_pred.extend(preds.cpu().numpy())\n\n        # 获取正类的概率值，假设是二分类，正类索引为 1\n        probs = F.softmax(logits, dim=1)[:, 1]\n        y_score.extend(probs.cpu().numpy())\n\n        y_true.extend(y_batch.cpu().numpy())\n\n\nfrom sklearn.metrics import (\n    precision_score, recall_score, f1_score, accuracy_score,\n    confusion_matrix, roc_auc_score\n)\n\ndef evaluate(y_true, y_pred, y_score=None):\n    precision = precision_score(y_true, y_pred, zero_division=0)\n    recall = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    acc = accuracy_score(y_true, y_pred)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    fpr = fp / (fp + tn + 1e-8)\n    pnr = fn / (fn + tp + 1e-8)\n    auc = roc_auc_score(y_true, y_score) if y_score is not None else None\n    return precision, recall, f1, acc, fpr, pnr, auc\n\n# 示例调用\nmetrics = evaluate(y_true, y_pred, y_score)\nnames = ['Precision', 'Recall', 'F1', 'Accuracy', 'FPR', 'PNR', 'AUC']\nfor n, v in zip(names, metrics):\n    print(f\"{n}: {v:.4f}\" if v is not None else f\"{n}: N/A\")\n\n# 训练直至收敛","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T08:15:20.939227Z","iopub.execute_input":"2025-07-17T08:15:20.939531Z","iopub.status.idle":"2025-07-17T08:15:49.875816Z","shell.execute_reply.started":"2025-07-17T08:15:20.939509Z","shell.execute_reply":"2025-07-17T08:15:49.875302Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 200/200 [00:26<00:00,  7.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 7.4614\nEpoch 2, Loss: 7.2101\nEpoch 3, Loss: 7.0998\nEpoch 4, Loss: 7.0111\nEpoch 5, Loss: 6.9111\nEpoch 6, Loss: 6.8478\nEpoch 7, Loss: 6.7958\nEpoch 8, Loss: 6.7296\nEpoch 9, Loss: 6.6771\nEpoch 10, Loss: 6.7051\nEpoch 11, Loss: 6.7089\nEpoch 12, Loss: 6.6354\nEpoch 13, Loss: 6.5739\nEpoch 14, Loss: 6.4849\nEpoch 15, Loss: 6.4727\nEpoch 16, Loss: 6.4478\nEpoch 17, Loss: 6.3221\nEpoch 18, Loss: 6.2659\nEpoch 19, Loss: 6.1971\nEpoch 20, Loss: 6.1885\nEpoch 21, Loss: 6.2925\nEpoch 22, Loss: 6.2430\nEpoch 23, Loss: 6.1701\nEpoch 24, Loss: 6.1611\nEpoch 25, Loss: 6.1658\nEpoch 26, Loss: 6.1327\nEpoch 27, Loss: 6.1692\nEpoch 28, Loss: 6.0921\nEpoch 29, Loss: 6.1057\nEpoch 30, Loss: 6.1032\nPrecision: 0.7308\nRecall: 0.6552\nF1: 0.6909\nAccuracy: 0.7167\nFPR: 0.2258\nPNR: 0.3448\nAUC: 0.7486\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"# 6. 只加入协同训练策略 【结论：0.15的惩罚系数要好一些，但提升的并不多】","metadata":{}},{"cell_type":"code","source":"# STEP 0: 引入 BERT 模块和 PCA\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.decomposition import PCA\nimport torch\nimport numpy as np\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\nfrom tqdm import tqdm\n\n# STEP 1: 加载 BERT 模型（一次性加载）\nbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nbert_model = BertModel.from_pretrained(\"bert-base-uncased\")\nbert_model.eval()  # 不训练\n\n# STEP 2: 获取 Explanation 的 BERT 向量\n@torch.no_grad()\ndef get_bert_embedding(text):\n    inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    outputs = bert_model(**inputs)\n    # 使用 [CLS] 向量作为整句表示\n    cls_embedding = outputs.last_hidden_state[:, 0, :]  # shape: [1, 768]\n    return cls_embedding.squeeze().numpy()  # shape: [768,]\n\n# 生成所有 Explanation 的 BERT 向量\nexplanation_embeddings = np.stack([get_bert_embedding(str(x)) for x in tqdm(df['all_Explanation'])])\n\n# STEP 3: 使用 PCA 降维将 BERT 向量从 768 维降到 8 维\npca = PCA(n_components=8)\nexplanation_embeddings_8d = pca.fit_transform(explanation_embeddings)  # shape: (200, 8)\n\n# STEP 4: Word2Vec 和数值特征的序列化\ndef row_to_sequence(row):\n    seq = []\n    for col in categorical_cols:\n        word = str(row[col])\n        vec = w2v_models[col].wv[word]\n        seq.append(vec)\n    for col in numerical_cols:\n        vec = np.full((embedding_size,), row[col])\n        seq.append(vec)\n    return np.stack(seq)  # shape: (len(categorical_cols + numerical_cols), embedding_size)\n\nX_seq_with_exp = []\n\nfor i in range(len(df)):\n    original_seq = row_to_sequence(df.iloc[i])  # shape: (seq_len, 8)\n    exp_vec_8d = explanation_embeddings_8d[i].reshape(1, -1)  # shape: (1, 8)\n    full_seq = np.concatenate([original_seq, exp_vec_8d], axis=0)  # shape: (seq_len + 1, 8) # 注意此处是直接进行拼接\n    X_seq_with_exp.append(full_seq)\n\nX_seq_with_exp = np.stack(X_seq_with_exp)\ny = df[target_col].values\n\n# STEP 5: 划分训练集\nX_train, X_test, y_train, y_test = train_test_split(X_seq_with_exp, y, test_size=0.3, random_state=42\n                                            #       , stratify=y\n                                                   )\n\n# STEP 6: 新的 Dataset 和 Dataloader\nclass SequenceDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self): return len(self.X)\n    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n\ntrain_loader = DataLoader(SequenceDataset(X_train, y_train), batch_size=16, shuffle=True)\ntest_loader = DataLoader(SequenceDataset(X_test, y_test), batch_size=16)\n\n# # ========== STEP 7: BiLSTM ==========\n\nimport torch.nn as nn\n\nclass BiLSTMWithBert(nn.Module):\n    def __init__(self, input_dim=8, exp_dim=8, hidden_dim=32, output_dim=2, num_layers=1):\n        super().__init__()\n        self.bilstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=True\n        )\n        self.linear_exp = nn.Linear(exp_dim, hidden_dim * 2)\n        self.fc = nn.Linear(hidden_dim * 4, output_dim)\n\n    def forward(self, x):\n        x_seq = x[:, :-1, :]   # (batch, seq_len, input_dim)\n        x_exp = x[:, -1, :]    # (batch, exp_dim)\n\n        # BiLSTM outputs\n        lstm_out, _ = self.bilstm(x_seq)      # (batch, seq_len, hidden_dim*2)\n        lstm_feat = lstm_out.mean(dim=1)      # Mean pooling over time\n\n        exp_feat = self.linear_exp(x_exp)     # (batch, hidden_dim*2)\n\n        combined = torch.cat([lstm_feat, exp_feat], dim=1)  # (batch, hidden_dim*4)\n        return self.fc(combined)\n\nmodel = BiLSTMWithBert(input_dim=8, exp_dim=8, hidden_dim=32, output_dim=2)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n\n\n\nimport torch.nn.functional as F\n\ndef cosine_similarity(a, b, eps=1e-8):\n    # 计算两个batch中对应样本的余弦相似度矩阵\n    # a,b: (batch_size, feat_dim)\n    a_norm = a / (a.norm(dim=1, keepdim=True) + eps)\n    b_norm = b / (b.norm(dim=1, keepdim=True) + eps)\n    sim_matrix = torch.matmul(a_norm, b_norm.t())  # (batch_size, batch_size)\n    return sim_matrix\n\nfor epoch in range(30):\n    model.train()\n    total_loss = 0\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        \n        # 取出原始序列和Explanation序列\n        x_seq = X_batch[:, :-1, :]  # 原始序列 (batch, seq_len, input_dim)\n        x_exp = X_batch[:, -1, :]   # Explanation 特征 (batch, exp_dim)\n        \n        # 模型正向输出\n        logits = model(X_batch)\n\n        # 计算交叉熵损失\n        ce_loss = criterion(logits, y_batch)\n\n        # 计算原始序列特征的平均池化作为原始embedding\n        original_seq_feat = x_seq.mean(dim=1)  # (batch, input_dim)\n\n        # 计算融合特征（这里用模型内部拼接前的特征，你也可以在 model.forward 改为输出这些特征）\n        lstm_out, _ = model.bilstm(x_seq)\n        lstm_feat = lstm_out.mean(dim=1)  # (batch, hidden_dim*2)\n        exp_feat = model.linear_exp(x_exp) # (batch, hidden_dim*2)\n        fused_feat = torch.cat([lstm_feat, exp_feat], dim=1)  # (batch, hidden_dim*4)\n\n        # 计算原始特征相似度矩阵 和 融合特征相似度矩阵\n        sim_orig = cosine_similarity(original_seq_feat, original_seq_feat)\n        sim_fused = cosine_similarity(fused_feat, fused_feat)\n\n        # 对角线元素相似度都是1，不用计算惩罚，可以mask掉\n        mask = ~torch.eye(sim_orig.size(0), dtype=bool)\n\n        # 计算差的绝对值（只计算非对角线部分）\n        consistency_loss = torch.abs(sim_orig - sim_fused)[mask].mean()\n\n        # 总损失 = 交叉熵 + 惩罚项 (惩罚项权重可调)\n        alpha = 0.2  # 可以调节惩罚权重\n        loss = ce_loss + alpha * consistency_loss\n\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n\n# STEP 9: 评估\nimport torch.nn.functional as F\n\nmodel.eval()\ny_true, y_pred, y_score = [], [], []\n\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        logits = model(X_batch)\n\n        # 获取预测标签（0或1）\n        preds = torch.argmax(logits, dim=1)\n        y_pred.extend(preds.cpu().numpy())\n\n        # 获取正类的概率值，假设是二分类，正类索引为 1\n        probs = F.softmax(logits, dim=1)[:, 1]\n        y_score.extend(probs.cpu().numpy())\n\n        y_true.extend(y_batch.cpu().numpy())\n\n\nfrom sklearn.metrics import (\n    precision_score, recall_score, f1_score, accuracy_score,\n    confusion_matrix, roc_auc_score\n)\n\ndef evaluate(y_true, y_pred, y_score=None):\n    precision = precision_score(y_true, y_pred, zero_division=0)\n    recall = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    acc = accuracy_score(y_true, y_pred)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    fpr = fp / (fp + tn + 1e-8)\n    pnr = fn / (fn + tp + 1e-8)\n    auc = roc_auc_score(y_true, y_score) if y_score is not None else None\n    return precision, recall, f1, acc, fpr, pnr, auc\n\n# 示例调用\nmetrics = evaluate(y_true, y_pred, y_score)\nnames = ['Precision', 'Recall', 'F1', 'Accuracy', 'FPR', 'PNR', 'AUC']\nfor n, v in zip(names, metrics):\n    print(f\"{n}: {v:.4f}\" if v is not None else f\"{n}: N/A\")\n\n# 训练直至收敛","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T07:20:55.025557Z","iopub.execute_input":"2025-07-17T07:20:55.025861Z","iopub.status.idle":"2025-07-17T07:21:27.044779Z","shell.execute_reply.started":"2025-07-17T07:20:55.025840Z","shell.execute_reply":"2025-07-17T07:21:27.044066Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 200/200 [00:29<00:00,  6.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 7.3648\nEpoch 2, Loss: 7.1461\nEpoch 3, Loss: 6.9315\nEpoch 4, Loss: 6.8031\nEpoch 5, Loss: 6.7796\nEpoch 6, Loss: 6.6917\nEpoch 7, Loss: 6.6875\nEpoch 8, Loss: 6.6176\nEpoch 9, Loss: 6.6056\nEpoch 10, Loss: 6.6109\nEpoch 11, Loss: 6.5359\nEpoch 12, Loss: 6.4769\nEpoch 13, Loss: 6.4965\nEpoch 14, Loss: 6.4069\nEpoch 15, Loss: 6.3250\nEpoch 16, Loss: 6.1957\nEpoch 17, Loss: 6.1249\nEpoch 18, Loss: 6.0755\nEpoch 19, Loss: 6.0608\nEpoch 20, Loss: 6.1008\nEpoch 21, Loss: 6.0041\nEpoch 22, Loss: 5.9926\nEpoch 23, Loss: 5.9878\nEpoch 24, Loss: 6.0299\nEpoch 25, Loss: 6.0199\nEpoch 26, Loss: 6.0207\nEpoch 27, Loss: 5.9196\nEpoch 28, Loss: 5.9769\nEpoch 29, Loss: 5.9530\nEpoch 30, Loss: 5.8823\nPrecision: 0.7500\nRecall: 0.6207\nF1: 0.6792\nAccuracy: 0.7167\nFPR: 0.1935\nPNR: 0.3793\nAUC: 0.7419\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# 7. 将协同学习与对比学习一起考虑","metadata":{}},{"cell_type":"markdown","source":"### MVP 最佳结果","metadata":{}},{"cell_type":"code","source":"# STEP 0: 引入 BERT 模块和 PCA\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.decomposition import PCA\nimport torch\nimport numpy as np\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\nfrom tqdm import tqdm\n\n# STEP 1: 加载 BERT 模型（一次性加载）\nbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nbert_model = BertModel.from_pretrained(\"bert-base-uncased\")\nbert_model.eval()  # 不训练\n\n# STEP 2: 获取 Explanation 的 BERT 向量\n@torch.no_grad()\ndef get_bert_embedding(text):\n    inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    outputs = bert_model(**inputs)\n    # 使用 [CLS] 向量作为整句表示\n    cls_embedding = outputs.last_hidden_state[:, 0, :]  # shape: [1, 768]\n    return cls_embedding.squeeze().numpy()  # shape: [768,]\n\n# 生成所有 Explanation 的 BERT 向量\nexplanation_embeddings = np.stack([get_bert_embedding(str(x)) for x in tqdm(df['all_Explanation'])])\n\n# STEP 3: 使用 PCA 降维将 BERT 向量从 768 维降到 8 维\npca = PCA(n_components=8)\nexplanation_embeddings_8d = pca.fit_transform(explanation_embeddings)  # shape: (200, 8)\n\n# STEP 4: Word2Vec 和数值特征的序列化\ndef row_to_sequence(row):\n    seq = []\n    for col in categorical_cols:\n        word = str(row[col])\n        vec = w2v_models[col].wv[word]\n        seq.append(vec)\n    for col in numerical_cols:\n        vec = np.full((embedding_size,), row[col])\n        seq.append(vec)\n    return np.stack(seq)  # shape: (len(categorical_cols + numerical_cols), embedding_size)\n\nX_seq_with_exp = []\n\nfor i in range(len(df)):\n    original_seq = row_to_sequence(df.iloc[i])  # shape: (seq_len, 8)\n    exp_vec_8d = explanation_embeddings_8d[i].reshape(1, -1)  # shape: (1, 8)\n    full_seq = np.concatenate([original_seq, exp_vec_8d], axis=0)  # shape: (seq_len + 1, 8) # 注意此处是直接进行拼接\n    X_seq_with_exp.append(full_seq)\n\nX_seq_with_exp = np.stack(X_seq_with_exp)\ny = df[target_col].values\n\n# STEP 5: 划分训练集\nX_train, X_test, y_train, y_test = train_test_split(X_seq_with_exp, y, test_size=0.3, random_state=42\n                                            #       , stratify=y\n                                                   )\n\n# STEP 6: 新的 Dataset 和 Dataloader\nclass SequenceDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self): return len(self.X)\n    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n\ntrain_loader = DataLoader(SequenceDataset(X_train, y_train), batch_size=16, shuffle=True)\ntest_loader = DataLoader(SequenceDataset(X_test, y_test), batch_size=16)\n\n# # ========== STEP 7: BiLSTM ==========\n\nimport torch.nn as nn\n\nclass BiLSTMWithBert(nn.Module):\n    def __init__(self, input_dim=8, exp_dim=8, hidden_dim=32, output_dim=2, num_layers=1):\n        super().__init__()\n        self.bilstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=True\n        )\n        self.linear_exp = nn.Linear(exp_dim, hidden_dim * 2)\n        self.fc = nn.Linear(hidden_dim * 4, output_dim)\n\n    def forward(self, x):\n        x_seq = x[:, :-1, :]   # (batch, seq_len, input_dim)\n        x_exp = x[:, -1, :]    # (batch, exp_dim)\n\n        # BiLSTM outputs\n        lstm_out, _ = self.bilstm(x_seq)      # (batch, seq_len, hidden_dim*2)\n        lstm_feat = lstm_out.mean(dim=1)      # Mean pooling over time\n\n        exp_feat = self.linear_exp(x_exp)     # (batch, hidden_dim*2)\n\n        combined = torch.cat([lstm_feat, exp_feat], dim=1)  # (batch, hidden_dim*4)\n        return self.fc(combined)\n\nmodel = BiLSTMWithBert(input_dim=8, exp_dim=8, hidden_dim=32, output_dim=2)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n\n# 定义对比损失regularizer \ndef info_nce_loss(features, labels, temperature=0.5):\n    \"\"\"\n    features: tensor of shape (batch_size, embed_dim)\n    labels: tensor of shape (batch_size,) with class labels\n    \"\"\"\n    # Normalize features\n    features = F.normalize(features, dim=1)  # cosine sim\n    similarity_matrix = torch.matmul(features, features.T)  # (batch, batch)\n\n    # Build labels mask (positive pairs: same label)\n    labels = labels.contiguous().view(-1, 1)\n    mask = torch.eq(labels, labels.T).float().to(features.device)  # (batch, batch)\n\n    # Avoid self-similarity\n    self_mask = torch.eye(mask.shape[0], device=features.device)\n    mask = mask - self_mask  # 1 for positive pairs only, 0 elsewhere\n\n    # Scale similarity\n    logits = similarity_matrix / temperature\n\n    # For numerical stability\n    logits_max, _ = torch.max(logits, dim=1, keepdim=True)\n    logits = logits - logits_max.detach()\n\n    exp_logits = torch.exp(logits) * (1 - self_mask)\n    log_prob = logits - torch.log(exp_logits.sum(dim=1, keepdim=True) + 1e-8)\n\n    # Compute mean of log-likelihood over positive\n    mean_log_prob_pos = (mask * log_prob).sum(dim=1) / (mask.sum(dim=1) + 1e-8)\n\n    loss = -mean_log_prob_pos.mean()\n    return loss\n\nimport torch.nn.functional as F\n\ndef cosine_similarity_matrix(features):\n    \"\"\"计算归一化后的余弦相似度矩阵\"\"\"\n    features = F.normalize(features, dim=1)\n    return torch.matmul(features, features.T)\n\nfor epoch in range(40):\n    model.train()\n    total_loss = 0\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        logits = model(X_batch)\n\n        # 原始分类损失\n        loss_cls = criterion(logits, y_batch)\n\n        # 中间表示提取\n        x_seq = X_batch[:, :-1, :]\n        x_exp = X_batch[:, -1, :]\n\n        # 原始序列平均池化作为 baseline 特征\n        original_feat = x_seq.mean(dim=1)  # (batch, input_dim)\n\n        # 模型的中间输出\n        lstm_out, _ = model.bilstm(x_seq)\n        lstm_feat = lstm_out.mean(dim=1)  # (batch, hidden_dim*2)\n        exp_feat = model.linear_exp(x_exp)  # (batch, hidden_dim*2)\n        fused_feat = torch.cat([lstm_feat, exp_feat], dim=1)  # (batch, hidden_dim*4)\n\n        # 对比损失 (InfoNCE)\n        loss_contrast = info_nce_loss(fused_feat, y_batch)\n\n        # 一致性惩罚项\n        sim_orig = cosine_similarity_matrix(original_feat)\n        sim_fused = cosine_similarity_matrix(fused_feat)\n        # 只保留非对角线项（mask掉自己和自己）\n        batch_size = sim_orig.shape[0]\n        mask = ~torch.eye(batch_size, dtype=bool, device=sim_orig.device)\n        loss_consistency = torch.abs(sim_orig - sim_fused)[mask].mean()\n\n        # 总损失\n        alpha = 0.05  # 对比损失系数 0.05\n        beta = 0.05    # 一致性损失系数，可调节 0.1\n        loss = loss_cls + alpha * loss_contrast + beta * loss_consistency\n        # alpha=0.01, beta=0.0 => Avg F1: 0.6486 交叉验证寻求最有组合\n\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n\n\n# STEP 9: 评估\nimport torch.nn.functional as F\n\nmodel.eval()\ny_true, y_pred, y_score = [], [], []\n\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        logits = model(X_batch)\n\n        # 获取预测标签（0或1）\n        preds = torch.argmax(logits, dim=1)\n        y_pred.extend(preds.cpu().numpy())\n\n        # 获取正类的概率值，假设是二分类，正类索引为 1\n        probs = F.softmax(logits, dim=1)[:, 1]\n        y_score.extend(probs.cpu().numpy())\n\n        y_true.extend(y_batch.cpu().numpy())\n\n\nfrom sklearn.metrics import (\n    precision_score, recall_score, f1_score, accuracy_score,\n    confusion_matrix, roc_auc_score\n)\n\ndef evaluate(y_true, y_pred, y_score=None):\n    precision = precision_score(y_true, y_pred, zero_division=0)\n    recall = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    acc = accuracy_score(y_true, y_pred)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    fpr = fp / (fp + tn + 1e-8)\n    pnr = fn / (fn + tp + 1e-8)\n    auc = roc_auc_score(y_true, y_score) if y_score is not None else None\n    return precision, recall, f1, acc, fpr, pnr, auc\n\n# 示例调用\nmetrics = evaluate(y_true, y_pred, y_score)\nnames = ['Precision', 'Recall', 'F1', 'Accuracy', 'FPR', 'PNR', 'AUC']\nfor n, v in zip(names, metrics):\n    print(f\"{n}: {v:.4f}\" if v is not None else f\"{n}: N/A\")\n\n# 训练直至收敛","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T08:49:54.970219Z","iopub.execute_input":"2025-07-17T08:49:54.970910Z","iopub.status.idle":"2025-07-17T08:50:28.925392Z","shell.execute_reply.started":"2025-07-17T08:49:54.970889Z","shell.execute_reply":"2025-07-17T08:50:28.924538Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 200/200 [00:30<00:00,  6.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 8.2075\nEpoch 2, Loss: 7.8507\nEpoch 3, Loss: 7.6621\nEpoch 4, Loss: 7.4385\nEpoch 5, Loss: 7.2928\nEpoch 6, Loss: 7.1990\nEpoch 7, Loss: 7.0681\nEpoch 8, Loss: 7.1212\nEpoch 9, Loss: 6.9872\nEpoch 10, Loss: 6.9861\nEpoch 11, Loss: 6.8845\nEpoch 12, Loss: 6.8004\nEpoch 13, Loss: 6.6687\nEpoch 14, Loss: 6.7165\nEpoch 15, Loss: 6.5371\nEpoch 16, Loss: 6.5153\nEpoch 17, Loss: 6.4307\nEpoch 18, Loss: 6.4717\nEpoch 19, Loss: 6.4324\nEpoch 20, Loss: 6.4137\nEpoch 21, Loss: 6.4955\nEpoch 22, Loss: 6.4108\nEpoch 23, Loss: 6.4465\nEpoch 24, Loss: 6.4927\nEpoch 25, Loss: 6.4622\nEpoch 26, Loss: 6.4046\nEpoch 27, Loss: 6.4274\nEpoch 28, Loss: 6.3459\nEpoch 29, Loss: 6.3708\nEpoch 30, Loss: 6.4067\nEpoch 31, Loss: 6.4265\nEpoch 32, Loss: 6.3441\nEpoch 33, Loss: 6.3654\nEpoch 34, Loss: 6.3877\nEpoch 35, Loss: 6.3324\nEpoch 36, Loss: 6.3767\nEpoch 37, Loss: 6.4324\nEpoch 38, Loss: 6.3640\nEpoch 39, Loss: 6.3934\nEpoch 40, Loss: 6.3490\nPrecision: 0.7692\nRecall: 0.6897\nF1: 0.7273\nAccuracy: 0.7500\nFPR: 0.1935\nPNR: 0.3103\nAUC: 0.7553\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"# 惩罚系数的调节","metadata":{}},{"cell_type":"code","source":"alpha_list = [0.0, 0.01, 0.05, 0.1]\nbeta_list = [0.0, 0.05, 0.1, 0.2]\n\nfrom sklearn.model_selection import KFold\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nresults = {}\n\nfor alpha in alpha_list:\n    for beta in beta_list:\n        fold_metrics = []\n\n        for fold, (train_idx, val_idx) in enumerate(kf.split(X_seq_with_exp)):\n            # 构造当前折的训练集和验证集\n            X_train_fold = X_seq_with_exp[train_idx]\n            y_train_fold = y[train_idx]\n            X_val_fold = X_seq_with_exp[val_idx]\n            y_val_fold = y[val_idx]\n\n            train_loader = DataLoader(SequenceDataset(X_train_fold, y_train_fold), batch_size=16, shuffle=True)\n            val_loader = DataLoader(SequenceDataset(X_val_fold, y_val_fold), batch_size=16)\n\n            # 初始化模型\n            model = BiLSTMWithBert(input_dim=8, exp_dim=8, hidden_dim=32, output_dim=2)\n            optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n            criterion = nn.CrossEntropyLoss()\n\n            # ---------- 简化版本训练 ----------\n            for epoch in range(5):  # 小轮数快速试验\n                model.train()\n                for X_batch, y_batch in train_loader:\n                    optimizer.zero_grad()\n                    logits = model(X_batch)\n                    loss_cls = criterion(logits, y_batch)\n\n                    x_seq = X_batch[:, :-1, :]\n                    x_exp = X_batch[:, -1, :]\n\n                    lstm_out, _ = model.bilstm(x_seq)\n                    lstm_feat = lstm_out.mean(dim=1)\n                    exp_feat = model.linear_exp(x_exp)\n                    fused_feat = torch.cat([lstm_feat, exp_feat], dim=1)\n                    original_feat = x_seq.mean(dim=1)\n\n                    loss_contrast = info_nce_loss(fused_feat, y_batch)\n                    sim_orig = F.normalize(original_feat, dim=1) @ F.normalize(original_feat, dim=1).T\n                    sim_fused = F.normalize(fused_feat, dim=1) @ F.normalize(fused_feat, dim=1).T\n                    mask = ~torch.eye(sim_orig.shape[0], dtype=bool)\n                    loss_consistency = torch.abs(sim_orig - sim_fused)[mask].mean()\n\n                    loss = loss_cls + alpha * loss_contrast + beta * loss_consistency\n                    loss.backward()\n                    optimizer.step()\n\n            # ---------- 验证评估 ----------\n            model.eval()\n            y_true, y_pred, y_score = [], [], []\n            with torch.no_grad():\n                for X_batch, y_batch in val_loader:\n                    logits = model(X_batch)\n                    probs = F.softmax(logits, dim=1)[:, 1]\n                    preds = torch.argmax(logits, dim=1)\n                    y_true.extend(y_batch.cpu().numpy())\n                    y_pred.extend(preds.cpu().numpy())\n                    y_score.extend(probs.cpu().numpy())\n\n            precision, recall, f1, acc, fpr, pnr, auc = evaluate(y_true, y_pred, y_score)\n            fold_metrics.append(auc)  # 你也可以换成其他指标如 auc\n\n        # 记录平均性能\n        avg_auc = sum(fold_metrics) / len(fold_metrics)\n        results[(alpha, beta)] = avg_auc\n        print(f\"alpha={alpha}, beta={beta} => Avg auc: {avg_auc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T08:38:07.702482Z","iopub.execute_input":"2025-07-17T08:38:07.703025Z","iopub.status.idle":"2025-07-17T08:38:39.676275Z","shell.execute_reply.started":"2025-07-17T08:38:07.703002Z","shell.execute_reply":"2025-07-17T08:38:39.675420Z"}},"outputs":[{"name":"stdout","text":"alpha=0.0, beta=0.0 => Avg auc: 0.6691\nalpha=0.0, beta=0.05 => Avg auc: 0.6863\nalpha=0.0, beta=0.1 => Avg auc: 0.7048\nalpha=0.0, beta=0.2 => Avg auc: 0.6321\nalpha=0.01, beta=0.0 => Avg auc: 0.6943\nalpha=0.01, beta=0.05 => Avg auc: 0.6563\nalpha=0.01, beta=0.1 => Avg auc: 0.6492\nalpha=0.01, beta=0.2 => Avg auc: 0.6713\nalpha=0.05, beta=0.0 => Avg auc: 0.6345\nalpha=0.05, beta=0.05 => Avg auc: 0.6978\nalpha=0.05, beta=0.1 => Avg auc: 0.6778\nalpha=0.05, beta=0.2 => Avg auc: 0.6658\nalpha=0.1, beta=0.0 => Avg auc: 0.6938\nalpha=0.1, beta=0.05 => Avg auc: 0.6638\nalpha=0.1, beta=0.1 => Avg auc: 0.6833\nalpha=0.1, beta=0.2 => Avg auc: 0.6778\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}